{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from model import Caption_Model\n",
    "from data_generator import Data_Generator\n",
    "from inference_util import Inference\n",
    "\n",
    "import inception_base\n",
    "import configuration\n",
    "import argparse\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "class Caption_Model(object):\n",
    "    def __init__(self,num_frames,\n",
    "                 image_width,\n",
    "                 image_height,\n",
    "                 image_channels,\n",
    "                 num_caption_unroll,\n",
    "                 num_last_layer_units,\n",
    "                 image_embedding_size,\n",
    "                 word_embedding_size,\n",
    "                 hidden_size_lstm,\n",
    "                 num_lstm_layer,\n",
    "                 vocab_size,\n",
    "                 initializer_scale,\n",
    "                 learning_rate,\n",
    "                 mode,\n",
    "                 rnn1_input_keep_prob ,\n",
    "                 rnn1_output_keep_prob ,\n",
    "                 rnn2_input_keep_prob ,\n",
    "                 rnn2_output_keep_prob ,\n",
    "                 ):\n",
    "        assert mode in [\"train\",\"val\",\"inference\"]\n",
    "        self.num_frames=num_frames\n",
    "        self.image_width=image_width\n",
    "        self.image_height=image_height\n",
    "        self.image_channels=image_channels\n",
    "        self.num_caption_unroll=num_caption_unroll\n",
    "        self.num_last_layer_units=num_last_layer_units\n",
    "        # self.image_embedding_size=image_embedding_size\n",
    "        self.word_embedding_size=word_embedding_size\n",
    "        self.hidden_size_lstm=hidden_size_lstm\n",
    "        self.num_lstm_layer = num_lstm_layer\n",
    "        self.vocab_size=vocab_size\n",
    "        self.initializer = tf.random_uniform_initializer(minval=-initializer_scale,maxval=initializer_scale)\n",
    "        self.learning_rate=learning_rate\n",
    "        self.mode=mode\n",
    "        self.rnn1_input_keep_prob=rnn1_input_keep_prob\n",
    "        self.rnn1_output_keep_prob=rnn1_output_keep_prob\n",
    "        self.rnn2_input_keep_prob=rnn2_input_keep_prob\n",
    "        self.rnn2_output_keep_prob=rnn2_output_keep_prob\n",
    "        self.summaries = []\n",
    "    def build(self):\n",
    "        self.build_inputs()\n",
    "        #self.build_inception_output()\n",
    "        self.build_embeddings()\n",
    "        self.build_train_op()\n",
    "        self.build_inference_op()\n",
    "        self.build_output_logits()\n",
    "        self.build_loss()\n",
    "        self.setup_global_step()\n",
    "        self.build_optimizer()\n",
    "        \n",
    "    def build_inputs(self):\n",
    "        with tf.variable_scope(\"inputs\") as scope:\n",
    "            self.processed_video_feed = tf.placeholder(tf.float32,\n",
    "                                        [None,self.num_frames,self.image_width,self.image_height,self.image_channels],\n",
    "                                        name=\"video_input\")\n",
    "            self.video_mask = tf.placeholder(tf.float32, [None, self.num_frames],name=\"video_mask\")\n",
    "            self.caption_input = tf.placeholder(tf.int32,[None,self.num_caption_unroll+1],name=\"caption_input\")\n",
    "            self.word_input = tf.placeholder(tf.int32,[None,1],name=\"word_input_inference\")\n",
    "            self.caption_mask = tf.placeholder(tf.float32,[None,self.num_caption_unroll],name=\"caption_mask\")\n",
    "            self.is_training = tf.placeholder(tf.bool, name='phase')\n",
    "            lengths = tf.add(tf.reduce_sum(self.caption_mask, 1), 1)\n",
    "            self.summaries.append(tf.summary.scalar(\"caption_length/batch_min\", tf.reduce_min(lengths)))\n",
    "            self.summaries.append(tf.summary.scalar(\"caption_length/batch_max\", tf.reduce_max(lengths)))\n",
    "            self.summaries.append(tf.summary.scalar(\"caption_length/batch_mean\", tf.reduce_mean(lengths)))\n",
    "\n",
    "    def build_inception_output(self):\n",
    "        interm_inputs = tf.reshape(self.processed_video_feed,[-1,self.image_width,self.image_height,self.image_channels])\n",
    "        inception_output = inception_base.get_base_model(interm_inputs)\n",
    "        self.inception_output = tf.reshape(inception_output,[-1,self.num_frames,self.num_last_layer_units])\n",
    "        self.inception_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"InceptionV4\") \n",
    "    def initialize_inception_pretrained(self,session,checkpoint_path):\n",
    "        saver = tf.train.Saver(var_list=self.inception_variables)\n",
    "        saver.restore(session,checkpoint_path)\n",
    "                                           \n",
    "    def build_embeddings(self):\n",
    "        with tf.variable_scope(\"inputs\") as scope:\n",
    "            self.rnn_input = tf.placeholder(dtype=tf.float32,\n",
    "                                            shape = [None,self.num_frames,self.num_last_layer_units],\n",
    "                                            name=\"rnn_input\")\n",
    "\n",
    "        # converting to shape (batch_size*num_frames,dim_output_inception)\n",
    "        interm_input = tf.reshape(self.rnn_input,[-1,self.num_last_layer_units])\n",
    "        \n",
    "        # with tf.variable_scope(\"image_encoding\") as scope:\n",
    "        #     h1 = tf.contrib.layers.fully_connected(inputs=interm_input,\n",
    "        #                                             num_outputs=self.image_embedding_size,\n",
    "        #                                             activation_fn = None,\n",
    "        #                                             weights_initializer=self.initializer,\n",
    "        #                                             biases_initializer=self.initializer,\n",
    "        #                                             scope = scope)\n",
    "        #     image_encoded = tf.contrib.layers.batch_norm(h1, \n",
    "        #                                     center=True, scale=True, \n",
    "        #                                     is_training=self.is_training,\n",
    "        #                                     scope=scope)\n",
    "        #     # image_encoded =  tf.nn.relu(h2, 'relu')\n",
    "            \n",
    "        #     # converting to shape (batch_size,num_frames,embedding_size)\n",
    "        #     self.image_encoded = tf.reshape(image_encoded,[-1,self.num_frames,self.image_embedding_size])\n",
    "        \n",
    "        with tf.variable_scope(\"embedding\") as scope:\n",
    "            self.word_emb = tf.get_variable(name=\"word_embedding\",\n",
    "                                            shape=[self.vocab_size, self.word_embedding_size],\n",
    "                                            initializer = self.initializer)\n",
    "        \n",
    "        with tf.variable_scope(\"rnn_units\"):\n",
    "            self.cell_1 = [tf.contrib.rnn.BasicLSTMCell(self.hidden_size_lstm,reuse=tf.get_variable_scope().reuse) for _ in range(self.num_lstm_layer)]\n",
    "            self.cell_2 = [tf.contrib.rnn.BasicLSTMCell(self.hidden_size_lstm,reuse=tf.get_variable_scope().reuse) for _ in range(self.num_lstm_layer)]\n",
    "        \n",
    "        self.caption_encoded = tf.nn.embedding_lookup(self.word_emb,self.caption_input,name=\"lookup_caption_embedding\")\n",
    "        #caption_encoded is of shape(batch_size,self.num_caption_unroll+1,self.word_embedding_size)\n",
    "\n",
    "        self.word_encoded = tf.nn.embedding_lookup(self.word_emb,self.word_input,name=\"lookup_word_embedding\")\n",
    "        #word_encoded is of shape(batch_size,1,self.word_embedding_size)\n",
    "        \n",
    "    def build_inference_op(self):\n",
    "        with tf.name_scope(\"Inference\"):\n",
    "            cell_1 = tf.contrib.rnn.MultiRNNCell(self.cell_1,state_is_tuple=True)\n",
    "            cell_2 = tf.contrib.rnn.MultiRNNCell(self.cell_2,state_is_tuple=True)\n",
    "            \n",
    "            ############################## INFERENCE GRAPH #############################################\n",
    "            with tf.name_scope(\"Video_Encoding\"):\n",
    "                with tf.variable_scope(\"RNN_1\",reuse=True) as scope:\n",
    "                    outputs_encoding, last_state_encoding = tf.nn.dynamic_rnn(\n",
    "                                                cell=cell_1,\n",
    "                                                # inputs=self.image_encoded,\n",
    "                                                inputs=self.rnn_input,\n",
    "                                                dtype=tf.float32)\n",
    "\n",
    "                batch_size_tensor = tf.shape(outputs_encoding)[0]\n",
    "                self.infer_hidden_state = tf.concat(last_state_encoding,axis=1,name=\"hidden_state_encoding_concat\")\n",
    "\n",
    "            self.state_feed = tf.placeholder(dtype=tf.float32,\n",
    "                                shape=[None, sum(cell_2.state_size)],\n",
    "                                name=\"state_feed\")\n",
    "\n",
    "            with tf.name_scope(\"Decoding_step\"):\n",
    "                state_tuple = tf.split(value=self.state_feed, num_or_size_splits=2, axis=1)\n",
    "                state_tuple = tf.contrib.rnn.LSTMStateTuple(*state_tuple)\n",
    "\n",
    "                batch_size_tensor = tf.shape(self.word_encoded)[0]\n",
    "                with tf.variable_scope(\"RNN_2\",reuse=True) as scope:\n",
    "                    outputs_decoding, last_state_decoding = tf.nn.dynamic_rnn(\n",
    "                                                cell=cell_2,\n",
    "                                                inputs=self.word_encoded,\n",
    "                                                initial_state=state_tuple,\n",
    "                                                dtype=tf.float32)\n",
    "\n",
    "                self.infer_last_state_decoding = tf.concat(last_state_decoding,axis=1,name=\"last_state_decoding_concat\")\n",
    "\n",
    "            self.infer_output_rnn = outputs_decoding\n",
    "            ############################################################################################\n",
    "    def build_train_op(self):\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            ############################## TRAINING GRAPH #############################################\n",
    "            # batch_size_tensor = tf.shape(self.image_encoded)[0]\n",
    "            batch_size_tensor = tf.shape(self.rnn_input)[0]\n",
    "\n",
    "            cell_1 = self.cell_1\n",
    "            cell_2 = self.cell_2\n",
    "\n",
    "            if self.mode==\"train\":\n",
    "                cell_1 = [ tf.contrib.rnn.DropoutWrapper( cell,\n",
    "                                                        input_keep_prob=self.rnn1_input_keep_prob,\n",
    "                                                        output_keep_prob=self.rnn1_output_keep_prob)\n",
    "                                                        for cell in cell_1]\n",
    "                cell_2 = [ tf.contrib.rnn.DropoutWrapper( cell,\n",
    "                                                        input_keep_prob=self.rnn2_input_keep_prob,\n",
    "                                                        output_keep_prob=self.rnn2_output_keep_prob)\n",
    "                                                        for cell in cell_2]\n",
    "\n",
    "            cell_1 = tf.contrib.rnn.MultiRNNCell(cell_1,state_is_tuple=True)\n",
    "            cell_2 = tf.contrib.rnn.MultiRNNCell(cell_2,state_is_tuple=True)\n",
    "            \n",
    "            with tf.name_scope(\"Encoding_stage\"):\n",
    "                sequence_length = tf.reduce_sum(self.video_mask, 1)\n",
    "                with tf.variable_scope(\"RNN_1\") as scope:            \n",
    "                    outputs_encoding,last_state_encoding = tf.nn.dynamic_rnn(\n",
    "                                                                cell=cell_1,\n",
    "                                                                sequence_length=sequence_length,\n",
    "                                                                # inputs=self.image_encoded,\n",
    "                                                                inputs=self.rnn_input,\n",
    "                                                                dtype=tf.float32)\n",
    "            with tf.name_scope(\"Decoding_state\"):\n",
    "                sequence_length = tf.reduce_sum(self.caption_mask, 1)\n",
    "                caption_needed = self.caption_encoded[:,:-1,:]\n",
    "                with tf.variable_scope(\"RNN_2\") as scope:            \n",
    "                    outputs_decoding,last_state_decoding = tf.nn.dynamic_rnn(\n",
    "                                                                cell = cell_2,\n",
    "                                                                inputs=caption_needed,\n",
    "                                                                sequence_length=sequence_length,\n",
    "                                                                dtype=tf.float32,\n",
    "                                                                initial_state=last_state_encoding)\n",
    "            self.train_output_rnn = outputs_decoding\n",
    "            ############################################################################################\n",
    "    def build_output_logits(self):\n",
    "        # converting to shape (batch_size*num_frames,dim_output_inception)\n",
    "        train_input = tf.reshape(self.train_output_rnn,[-1,self.hidden_size_lstm])\n",
    "        infer_input = tf.reshape(self.infer_output_rnn,[-1,self.hidden_size_lstm])\n",
    "        \n",
    "        with tf.variable_scope(\"word_decoding\") as scope:\n",
    "            train_logits_decoded = tf.contrib.layers.fully_connected(inputs=train_input,\n",
    "                                                          num_outputs=self.vocab_size,\n",
    "                                                          activation_fn = None,\n",
    "                                                          weights_initializer=self.initializer,\n",
    "                                                          biases_initializer=self.initializer,\n",
    "                                                          scope = scope)\n",
    "            scope.reuse_variables()\n",
    "            infer_logits_decoded = tf.contrib.layers.fully_connected(inputs=infer_input,\n",
    "                                                          num_outputs=self.vocab_size,\n",
    "                                                          activation_fn = None,\n",
    "                                                          weights_initializer=self.initializer,\n",
    "                                                          biases_initializer=self.initializer,\n",
    "                                                          scope = scope)\n",
    "\n",
    "            # as the output will be a single word\n",
    "            self.infer_logits = tf.reshape(infer_logits_decoded,[-1,1,self.vocab_size])\n",
    "            self.infer_predictions = tf.nn.softmax(self.infer_logits)\n",
    "\n",
    "            self.train_logits = tf.reshape(train_logits_decoded,[-1,self.num_caption_unroll,self.vocab_size])\n",
    "            self.train_predictions = tf.nn.softmax(self.train_logits)\n",
    "\n",
    "    def build_loss(self):\n",
    "        with tf.variable_scope(\"loss\") as scope:\n",
    "            correct_predictions = self.caption_input[:,1:] #shape = (batch_size,num_caption_unroll) int32\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=correct_predictions,logits=self.train_logits)\n",
    "            #loss of shape (batch_size,num_caption_unroll) float32\n",
    "\n",
    "            # Irrelevant now. But leaving for fun's sake ;)\n",
    "            #    # I belive that this should be\n",
    "            #    #    self.correct_loss = tf.multiply(self.loss, self.caption_mask[:, 1:]\n",
    "            #    # because caption_mask denotes whether the ith word was actual caption or padding and as loss\n",
    "            #    # is calculated using the prediction of next word thus we should multiply it by whether the next\n",
    "            #    # word was padding or not and not on whether the current word was padding or not\n",
    "\n",
    "            self.loss = tf.multiply(loss, self.caption_mask) #shape = (batch_size,num_caption_unroll)\n",
    "            \n",
    "            batch_size_tensor = tf.shape(self.loss)[0]\n",
    "\n",
    "            self.individual_loss = tf.reduce_sum(self.loss,axis=1) #shape = (batch_size,)\n",
    "            \n",
    "            #### Which loss to use?\n",
    "            ## Sum of loss of all the words in a sequence (Done in s2vt code that we saw)\n",
    "            #self.batch_loss = tf.reduce_sum(self.loss)/tf.to_float(batch_size_tensor)\n",
    "            #self.batch_loss = tf.reduce_mean(self.individual_loss)\n",
    "            \n",
    "            ## Avg of loss of all the words in a sequence (Dome in im2txt code that we saw)\n",
    "            self.batch_loss = tf.div(tf.reduce_sum(self.loss),\n",
    "                                    tf.reduce_sum(self.caption_mask),\n",
    "                                    name=\"batch_loss\")\n",
    "\n",
    "            self.summaries.append(tf.summary.scalar('Batch_Loss', self.batch_loss))\n",
    "            self.summaries.append(tf.summary.histogram('Loss_Histogram', self.batch_loss))\n",
    "    def build_optimizer(self):\n",
    "        with tf.name_scope(\"Optimizer\") as scope:\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.train_step = optimizer.minimize(self.batch_loss,global_step=self.global_step)\n",
    "    def setup_global_step(self):\n",
    "        \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "        global_step = tf.Variable(\n",
    "            initial_value=0,\n",
    "            name=\"global_step\",\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "        self.global_step = global_step\n",
    "        \n",
    "\n",
    "data_config = configuration.DataConfig().config\n",
    "data_gen = Data_Generator(processed_video_dir = data_config[\"processed_video_dir\"],\n",
    "                         caption_file = data_config[\"caption_file\"],\n",
    "                         unique_freq_cutoff = data_config[\"unique_frequency_cutoff\"],\n",
    "                         max_caption_len = data_config[\"max_caption_length\"])\n",
    "\n",
    "data_gen.load_vocabulary(data_config[\"caption_data_dir\"])\n",
    "data_gen.load_dataset(data_config[\"caption_data_dir\"])\n",
    "#data_gen.build_dataset()\n",
    "\n",
    "model_config = configuration.ModelConfig(data_gen).config\n",
    "model = Caption_Model( num_frames = model_config[\"num_frames\"],\n",
    "                    image_width = model_config[\"image_width\"],\n",
    "                    image_height = model_config[\"image_height\"],\n",
    "                    image_channels = model_config[\"image_channels\"],\n",
    "                    num_caption_unroll = model_config[\"num_caption_unroll\"],\n",
    "                    num_last_layer_units = model_config[\"num_last_layer_units\"],\n",
    "                    image_embedding_size = model_config[\"image_embedding_size\"],\n",
    "                    word_embedding_size = model_config[\"word_embedding_size\"],\n",
    "                    hidden_size_lstm = model_config[\"hidden_size_lstm\"],\n",
    "                    num_lstm_layer = model_config[\"num_lstm_layer\"],\n",
    "                    vocab_size = model_config[\"vocab_size\"],\n",
    "                    initializer_scale = model_config[\"initializer_scale\"],\n",
    "                    learning_rate = model_config[\"learning_rate\"],\n",
    "                    mode=\"train\",\n",
    "                    rnn1_input_keep_prob=model_config[\"rnn1_input_keep_prob\"],\n",
    "                    rnn1_output_keep_prob=model_config[\"rnn1_output_keep_prob\"],\n",
    "                    rnn2_input_keep_prob=model_config[\"rnn2_input_keep_prob\"],\n",
    "                    rnn2_output_keep_prob=model_config[\"rnn2_output_keep_prob\"]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'num_lstm_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c248e30b1a55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_lstm_layer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'num_lstm_layer'"
     ]
    }
   ],
   "source": [
    "model_config[\"num_lstm_layer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
