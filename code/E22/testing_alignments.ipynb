{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "from model import Caption_Model\n",
    "from data_generator import Data_Generator\n",
    "from inference_util import Inference\n",
    "\n",
    "import inception_base\n",
    "import configuration\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                       \"Batch size of train data input.\")\n",
    "tf.flags.DEFINE_integer(\"beam_size\", 3,\n",
    "                       \"Beam size.\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_model\", \"./models/train/model-11380\",\n",
    "                       \"Model Checkpoint to use.\")\n",
    "tf.flags.DEFINE_integer(\"max_captions\", 3,\n",
    "                       \"Maximum number of captions to generate\")\n",
    "tf.flags.DEFINE_integer(\"max_len_captions\", 20,\n",
    "                       \"Maximum length of captions to generate\")\n",
    "tf.flags.DEFINE_string(\"dataset\", \"test\",\n",
    "                       \"Dataset to use\")\n",
    "tf.flags.DEFINE_string(\"outfile_name\", \"generated_caption_tt.json\",\n",
    "                       \"Name of the output result file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    FLAGS._parse_flags()\n",
    "    data_config = configuration.DataConfig().config\n",
    "\n",
    "    if FLAGS.checkpoint_model:\n",
    "        model_path = FLAGS.checkpoint_model\n",
    "    else:\n",
    "        model_path = tf.train.latest_checkpoint(data_config[\"checkpoint_dir\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_paths = [model_path]\n",
    "dataset = FLAGS.dataset\n",
    "batch_size = FLAGS.batch_size\n",
    "max_len_captions =FLAGS.max_len_captions\n",
    "max_captions = FLAGS.max_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same number of elements. First structure: AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'Inference/Decoding_step/tile_batch_1/Reshape:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Inference/Decoding_step/tile_batch_2/Reshape:0' shape=(?, 500) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'Inference/Decoding_step/tile_batch_3/Reshape:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Inference/Decoding_step/tile_batch_4/Reshape:0' shape=(?, 500) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'Inference/Decoding_step/tile_batch_5/Reshape:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Inference/Decoding_step/tile_batch_6/Reshape:0' shape=(?, 500) dtype=float32>)), attention=<tf.Tensor 'Inference/Decoding_step/AttentionWrapperZeroState/zeros_1:0' shape=(?, 200) dtype=float32>, time=<tf.Tensor 'Inference/Decoding_step/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'Inference/Decoding_step/AttentionWrapperZeroState/zeros_2:0' shape=(?, 100) dtype=float32>, alignment_history=<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f2d898c27f0>), second structure: AttentionWrapperState(cell_state=(LSTMStateTuple(c=500, h=500), LSTMStateTuple(c=500, h=500), LSTMStateTuple(c=500, h=500)), attention=200, time=TensorShape([]), alignments=100, alignment_history=()).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-50ec32f578e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"beam_width\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCaption_Model\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0minfer_util\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ozym4nd145/Coding/Notebook/SURA/code/E22/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_train_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inference_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_output_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ozym4nd145/Coding/Notebook/SURA/code/E22/model.py\u001b[0m in \u001b[0;36mbuild_inference_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m                                     \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_decoder_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                                     \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                                     output_layer=self.rnn_transform_layer)\n\u001b[0m\u001b[1;32m    203\u001b[0m                     outputs_decoding, last_state_decoding, _ = tf.contrib.seq2seq.dynamic_decode(\n\u001b[1;32m    204\u001b[0m                                                             decoder,maximum_iterations=self.num_caption_unroll)\n",
      "\u001b[0;32m/home/ozym4nd145/.local/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight)\u001b[0m\n\u001b[1;32m    191\u001b[0m     self._initial_cell_state = nest.map_structure(\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_split_batch_beams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         initial_state, self._cell.state_size)\n\u001b[0m\u001b[1;32m    194\u001b[0m     self._start_tokens = array_ops.tile(\n\u001b[1;32m    195\u001b[0m         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])\n",
      "\u001b[0;32m/home/ozym4nd145/.local/lib/python3.5/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[0mflat_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ozym4nd145/.local/lib/python3.5/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[0;34m(nest1, nest2, check_types)\u001b[0m\n\u001b[1;32m    144\u001b[0m     raise ValueError(\"The two structures don't have the same number of \"\n\u001b[1;32m    145\u001b[0m                      \u001b[0;34m\"elements. First structure: %s, second structure: %s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                      % (nest1, nest2))\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0m_recursive_assert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The two structures don't have the same number of elements. First structure: AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'Inference/Decoding_step/tile_batch_1/Reshape:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Inference/Decoding_step/tile_batch_2/Reshape:0' shape=(?, 500) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'Inference/Decoding_step/tile_batch_3/Reshape:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Inference/Decoding_step/tile_batch_4/Reshape:0' shape=(?, 500) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'Inference/Decoding_step/tile_batch_5/Reshape:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Inference/Decoding_step/tile_batch_6/Reshape:0' shape=(?, 500) dtype=float32>)), attention=<tf.Tensor 'Inference/Decoding_step/AttentionWrapperZeroState/zeros_1:0' shape=(?, 200) dtype=float32>, time=<tf.Tensor 'Inference/Decoding_step/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'Inference/Decoding_step/AttentionWrapperZeroState/zeros_2:0' shape=(?, 100) dtype=float32>, alignment_history=<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f2d898c27f0>), second structure: AttentionWrapperState(cell_state=(LSTMStateTuple(c=500, h=500), LSTMStateTuple(c=500, h=500), LSTMStateTuple(c=500, h=500)), attention=200, time=TensorShape([]), alignments=100, alignment_history=())."
     ]
    }
   ],
   "source": [
    "        data_config = configuration.DataConfig().config\n",
    "        data_gen = Data_Generator(processed_video_dir = data_config[\"processed_video_dir\"],\n",
    "                                caption_file = data_config[\"caption_file\"],\n",
    "                                unique_freq_cutoff = data_config[\"unique_frequency_cutoff\"],\n",
    "                                max_caption_len = data_config[\"max_caption_length\"])\n",
    "\n",
    "        data_gen.load_vocabulary(data_config[\"caption_data_dir\"])\n",
    "        data_gen.load_dataset(data_config[\"caption_data_dir\"])\n",
    "\n",
    "        assert dataset in [\"val\",\"test\",\"train\"]\n",
    "\n",
    "        if max_len_captions:\n",
    "            max_len = max_len_captions\n",
    "        else:\n",
    "            max_len = data_config['max_caption_length']\n",
    "\n",
    "        model_config = configuration.ModelConfig(data_gen).config\n",
    "        model_config[\"beam_width\"] = FLAGS.beam_size\n",
    "        model = Caption_Model( **model_config,mode=\"inference\")\n",
    "        model.build()\n",
    "\n",
    "        infer_util = Inference(model,data_gen.word_to_idx,data_gen.idx_to_word)\n",
    "\n",
    "        if max_captions:\n",
    "            max_iter = max_captions\n",
    "        else:\n",
    "            max_iter = len(data_gen.dataset[dataset])+10 #+10 is just to be safe ;)\n",
    "        \n",
    "        video_paths = {i[\"file_name\"]:i[\"path\"] for i in data_gen.dataset[dataset]}\n",
    "        \n",
    "        \n",
    "        gen_captions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import inception_base\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.util import nest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Caption_Model(object):\n",
    "    def __init__(self,num_frames,\n",
    "                 image_width,\n",
    "                 image_height,\n",
    "                 image_channels,\n",
    "                 num_caption_unroll,\n",
    "                 num_last_layer_units,\n",
    "                 word_embedding_size,\n",
    "                 hidden_size_lstm,\n",
    "                 num_lstm_layer,\n",
    "                 vocab_size,\n",
    "                 initializer_scale,\n",
    "                 learning_rate,\n",
    "                 mode,\n",
    "                 rnn1_input_keep_prob ,\n",
    "                 rnn1_output_keep_prob ,\n",
    "                 rnn2_input_keep_prob ,\n",
    "                 rnn2_output_keep_prob ,\n",
    "                 embedding_file,\n",
    "                 attention_num_units,\n",
    "                 attention_layer_units,\n",
    "                 beam_width,\n",
    "                 start_token_id,\n",
    "                 end_token_id,\n",
    "                 ):\n",
    "        assert mode in [\"train\",\"val\",\"inference\"]\n",
    "        self.num_frames=num_frames\n",
    "        self.image_width=image_width\n",
    "        self.image_height=image_height\n",
    "        self.image_channels=image_channels\n",
    "        self.num_caption_unroll=num_caption_unroll\n",
    "        self.num_last_layer_units=num_last_layer_units\n",
    "        # self.image_embedding_size=image_embedding_size\n",
    "        self.word_embedding_size=word_embedding_size\n",
    "        self.hidden_size_lstm=hidden_size_lstm\n",
    "        self.num_lstm_layer = num_lstm_layer\n",
    "        self.vocab_size=vocab_size\n",
    "        self.initializer = tf.random_uniform_initializer(minval=-initializer_scale,maxval=initializer_scale)\n",
    "        self.learning_rate=learning_rate\n",
    "        self.mode=mode\n",
    "        self.rnn1_input_keep_prob=rnn1_input_keep_prob\n",
    "        self.rnn1_output_keep_prob=rnn1_output_keep_prob\n",
    "        self.rnn2_input_keep_prob=rnn2_input_keep_prob\n",
    "        self.rnn2_output_keep_prob=rnn2_output_keep_prob\n",
    "        self.embedding_file = embedding_file\n",
    "        self.attention_num_units = attention_num_units\n",
    "        self.attention_layer_units = attention_layer_units\n",
    "        self.summaries = []\n",
    "        self.beam_width = beam_width\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "    def build(self):\n",
    "        self.build_inputs()\n",
    "        #self.build_inception_output()\n",
    "        self.build_embeddings()\n",
    "#         self.build_train_op()\n",
    "#         self.build_inference_op()\n",
    "#         self.build_output_logits()\n",
    "#         self.build_loss()\n",
    "#         self.setup_global_step()\n",
    "#         self.build_optimizer()\n",
    "        \n",
    "    def build_inputs(self):\n",
    "        with tf.variable_scope(\"inputs\") as scope:\n",
    "            self.processed_video_feed = tf.placeholder(tf.float32,\n",
    "                                        [None,self.num_frames,self.image_width,self.image_height,self.image_channels],\n",
    "                                        name=\"video_input\")\n",
    "            self.video_mask = tf.placeholder(tf.int32, [None, self.num_frames],name=\"video_mask\")\n",
    "            self.caption_input = tf.placeholder(tf.int32,[None,self.num_caption_unroll+1],name=\"caption_input\")\n",
    "            self.caption_mask = tf.placeholder(tf.int32,[None,self.num_caption_unroll],name=\"caption_mask\")\n",
    "            self.is_training = tf.placeholder(tf.bool, name='phase')\n",
    "            lengths = tf.add(tf.reduce_sum(self.caption_mask, 1), 1)\n",
    "            self.summaries.append(tf.summary.scalar(\"caption_length/batch_min\", tf.reduce_min(lengths)))\n",
    "            self.summaries.append(tf.summary.scalar(\"caption_length/batch_max\", tf.reduce_max(lengths)))\n",
    "            self.summaries.append(tf.summary.scalar(\"caption_length/batch_mean\", tf.reduce_mean(lengths)))\n",
    "\n",
    "    def build_inception_output(self):\n",
    "        interm_inputs = tf.reshape(self.processed_video_feed,[-1,self.image_width,self.image_height,self.image_channels])\n",
    "        inception_output = inception_base.get_base_model(interm_inputs)\n",
    "        self.inception_output = tf.reshape(inception_output,[-1,self.num_frames,self.num_last_layer_units])\n",
    "        self.inception_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"InceptionV4\") \n",
    "    def initialize_inception_pretrained(self,session,checkpoint_path):\n",
    "        saver = tf.train.Saver(var_list=self.inception_variables)\n",
    "        saver.restore(session,checkpoint_path)\n",
    "                                           \n",
    "    def build_embeddings(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope(\"inputs\") as scope:\n",
    "                self.rnn_input = tf.placeholder(dtype=tf.float32,\n",
    "                                                shape = [None,self.num_frames,self.num_last_layer_units],\n",
    "                                                name=\"rnn_input\")\n",
    "\n",
    "            # converting to shape (batch_size*num_frames,dim_output_inception)\n",
    "            interm_input = tf.reshape(self.rnn_input,[-1,self.num_last_layer_units])\n",
    "            \n",
    "            # with tf.variable_scope(\"image_encoding\") as scope:\n",
    "            #     h1 = tf.contrib.layers.fully_connected(inputs=interm_input,\n",
    "            #                                             num_outputs=self.image_embedding_size,\n",
    "            #                                             activation_fn = None,\n",
    "            #                                             weights_initializer=self.initializer,\n",
    "            #                                             biases_initializer=self.initializer,\n",
    "            #                                             scope = scope)\n",
    "            #     image_encoded = tf.contrib.layers.batch_norm(h1, \n",
    "            #                                     center=True, scale=True, \n",
    "            #                                     is_training=self.is_training,\n",
    "            #                                     scope=scope)\n",
    "            #     # image_encoded =  tf.nn.relu(h2, 'relu')\n",
    "                \n",
    "            #     # converting to shape (batch_size,num_frames,embedding_size)\n",
    "            #     self.image_encoded = tf.reshape(image_encoded,[-1,self.num_frames,self.image_embedding_size])\n",
    "            \n",
    "            with tf.variable_scope(\"embedding\") as scope:\n",
    "                ## Pretrained Variable Embedding\n",
    "                self.special_emb = tf.get_variable(name=\"spc_embedding\",\n",
    "                                                shape=[4, self.word_embedding_size],\n",
    "                                                initializer = self.initializer)\n",
    "                self.pretrained_emb = tf.Variable(initial_value=np.load(self.embedding_file),\n",
    "                                            dtype=tf.float32,name=\"glove_embedding\",trainable=True)\n",
    "                self.word_embedding = tf.concat([self.special_emb,self.pretrained_emb],axis=0, name=\"word_embedding\")\n",
    "                \n",
    "                ## Pretrained Constant Embedding\n",
    "                # self.word_emb = tf.constant(np.load(self.embedding_file),\n",
    "                #                             shape=[self.vocab_size-4, self.word_embedding_size],\n",
    "                #                             dtype=tf.float32,name=\"glove_embedding\",\n",
    "                #                             verify_shape=True)\n",
    "                # self.word_emb = tf.placeholder(dtype=tf.float32, name=\"word_embedding\",\n",
    "                #                                 shape=[self.vocab_size-4, self.word_embedding_size]\n",
    "                #                                 )\n",
    "                \n",
    "                ## Untrained embedding\n",
    "                # self.word_embedding = tf.get_variable(dtype=tf.float32, name=\"word_embedding\",\n",
    "                #                                 shape=[self.vocab_size, self.word_embedding_size]\n",
    "                #                                 )\n",
    "            \n",
    "            with tf.variable_scope(\"rnn_units\"):\n",
    "                self.cell_1 = [tf.contrib.rnn.BasicLSTMCell(self.hidden_size_lstm,reuse=tf.get_variable_scope().reuse) for _ in range(self.num_lstm_layer)]\n",
    "                self.cell_2 = [tf.contrib.rnn.BasicLSTMCell(self.hidden_size_lstm,reuse=tf.get_variable_scope().reuse) for _ in range(self.num_lstm_layer)]\n",
    "            \n",
    "            self.caption_encoded = tf.nn.embedding_lookup(self.word_embedding,self.caption_input,name=\"lookup_caption_embedding\")\n",
    "            #caption_encoded is of shape(batch_size,self.num_caption_unroll+1,self.word_embedding_size)\n",
    "\n",
    "            self.rnn_transform_layer = Dense(self.vocab_size)\n",
    "    def build_inference_op(self):\n",
    "        with tf.name_scope(\"Inference\"):\n",
    "        \tpass\n",
    "            # cell_1 = tf.contrib.rnn.MultiRNNCell(self.cell_1,state_is_tuple=True)\n",
    "            # cell_2 = tf.contrib.rnn.MultiRNNCell(self.cell_2,state_is_tuple=True)\n",
    "            \n",
    "            # ############################## INFERENCE GRAPH #############################################\n",
    "            # with tf.name_scope(\"Video_Encoding\"):\n",
    "            #     with tf.variable_scope(\"RNN_1\",reuse=True) as scope:\n",
    "            #         outputs_encoding, last_state_encoding = tf.nn.dynamic_rnn(\n",
    "            #                                     cell=cell_1,\n",
    "            #                                     # inputs=self.image_encoded,\n",
    "            #                                     inputs=self.rnn_input,\n",
    "            #                                     dtype=tf.float32)\n",
    "\n",
    "            #     batch_size_tensor = tf.shape(outputs_encoding)[0]\n",
    "            #     self.infer_hidden_state = last_state_encoding\n",
    "\n",
    "            # with tf.name_scope(\"Decoding_step\"):\n",
    "            #     encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
    "            #                         outputs_encoding, multiplier=self.beam_width)\n",
    "            #     encoder_last_state = nest.map_structure(\n",
    "            #                 lambda s: tf.contrib.seq2seq.tile_batch(s, self.beam_width), last_state_encoding)\n",
    "                # with tf.variable_scope(\"Attention\",reuse=True) as scope:\n",
    "                #     attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                #                     num_units = self.attention_num_units, # depth of query mechanism\n",
    "                #                     memory = encoder_outputs, # hidden states to attend (output of RNN)\n",
    "                #                     #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories\n",
    "                #                     normalize=False, # normalize energy term\n",
    "                #                     name='BahdanauAttention')\n",
    "                #     attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                #                     cell = cell_2,# Instance of RNNCell\n",
    "                #                     attention_mechanism = attn_mech, # Instance of AttentionMechanism\n",
    "                #                     attention_layer_size = self.attention_layer_units, # Int, depth of attention (output) tensor\n",
    "                #                     alignment_history=False, # whether to store history in final output\n",
    "                #                     output_attention=False,\n",
    "                #                     name=\"attention_wrapper\")\n",
    "\n",
    "                # start_tokens = tf.fill(dims=[batch_size_tensor],value=self.start_token_id)\n",
    "                # attn_zero_state = attn_cell.zero_state(batch_size=batch_size_tensor*self.beam_width,dtype=tf.float32)\n",
    "                # initial_decoder_state = attn_zero_state.clone(cell_state=encoder_last_state)\n",
    "\n",
    "                # with tf.variable_scope(\"RNN_2\",reuse=True) as scope:\n",
    "                #     decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                #                     cell=attn_cell,\n",
    "                #                     embedding=self.word_embedding,\n",
    "                #                     start_tokens=start_tokens,\n",
    "                #                     end_token=self.end_token_id,\n",
    "                #                     initial_state=initial_decoder_state,\n",
    "                #                     beam_width=self.beam_width,\n",
    "                #                     output_layer=self.rnn_transform_layer)\n",
    "                #     outputs_decoding, last_state_decoding, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                #                                             decoder,maximum_iterations=self.num_caption_unroll)\n",
    "                # self.infer_last_state_decoding = last_state_decoding\n",
    "                # self.infer_output_rnn = outputs_decoding\n",
    "            ############################################################################################\n",
    "    def build_output_logits(self):\n",
    "            # gives directly the word ids, is of shape [batch_size,sentence_length,beam_size]\n",
    "            # self.infer_predictions = self.infer_output_rnn.predicted_ids\n",
    "            self.train_predictions = tf.nn.softmax(self.train_output_rnn)\n",
    "\n",
    "    def build_loss(self):\n",
    "        with tf.variable_scope(\"loss\") as scope:\n",
    "            self.max_decoder_length = tf.reduce_max(self.train_output_len)\n",
    "            correct_predictions = self.caption_input[:,1:self.max_decoder_length+1] #shape = (batch_size,self.max_decoder_length) int32\n",
    "            self.mask = tf.sequence_mask(lengths=tf.reduce_sum(self.caption_mask, 1),\n",
    "                                    maxlen=self.max_decoder_length,dtype=tf.float32,name=\"caption_mask\")\n",
    "            self.train_pred = tf.argmax(self.train_output_rnn, axis=-1,\n",
    "                                                    name='decoder_pred_train')\n",
    "            self.batch_loss = tf.contrib.seq2seq.sequence_loss(logits=self.train_output_rnn,\n",
    "                                                                targets=correct_predictions,\n",
    "                                                                weights=self.mask,\n",
    "                                                                average_across_timesteps=True,\n",
    "                                                                average_across_batch=True)\n",
    "            self.summaries.append(tf.summary.scalar('Batch_Loss', self.batch_loss))\n",
    "            self.summaries.append(tf.summary.histogram('Loss_Histogram', self.batch_loss))\n",
    "            for var in tf.trainable_variables():\n",
    "                self.summaries.append(tf.summary.histogram(\"parameters/\" + var.op.name, var))\n",
    "\n",
    "    def build_optimizer(self):\n",
    "        with tf.name_scope(\"Optimizer\") as scope:\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                self.train_step = optimizer.minimize(self.batch_loss,global_step=self.global_step)\n",
    "    def setup_global_step(self):\n",
    "        \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "        global_step = tf.Variable(\n",
    "            initial_value=0,\n",
    "            name=\"global_step\",\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "        self.global_step = global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configuration\n",
    "from data_generator import Data_Generator\n",
    "from inference_util import Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    data_config = configuration.DataConfig().config\n",
    "    data_gen = Data_Generator(processed_video_dir = data_config[\"processed_video_dir\"],\n",
    "                             caption_file = data_config[\"caption_file\"],\n",
    "                             unique_freq_cutoff = data_config[\"unique_frequency_cutoff\"],\n",
    "                             max_caption_len = data_config[\"max_caption_length\"])\n",
    "\n",
    "    data_gen.load_vocabulary(data_config[\"caption_data_dir\"])\n",
    "    data_gen.load_dataset(data_config[\"caption_data_dir\"])\n",
    "    #data_gen.build_dataset()\n",
    "    model_config = configuration.ModelConfig(data_gen).config\n",
    "    model = Caption_Model(**model_config,mode=\"train\")\n",
    "    model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "self = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Train\"):\n",
    "    batch_size_tensor = tf.shape(self.rnn_input)[0]\n",
    "\n",
    "    cell_1 = self.cell_1\n",
    "    cell_2 = self.cell_2\n",
    "\n",
    "    if self.mode==\"train\":\n",
    "        cell_1 = [ tf.contrib.rnn.DropoutWrapper( cell,\n",
    "                                                input_keep_prob=self.rnn1_input_keep_prob,\n",
    "                                                output_keep_prob=self.rnn1_output_keep_prob)\n",
    "                                                for cell in cell_1]\n",
    "        cell_2 = [ tf.contrib.rnn.DropoutWrapper( cell,\n",
    "                                                input_keep_prob=self.rnn2_input_keep_prob,\n",
    "                                                output_keep_prob=self.rnn2_output_keep_prob)\n",
    "                                                for cell in cell_2]\n",
    "\n",
    "    cell_1 = tf.contrib.rnn.MultiRNNCell(cell_1,state_is_tuple=True)\n",
    "    cell_2 = tf.contrib.rnn.MultiRNNCell(cell_2,state_is_tuple=True)\n",
    "\n",
    "    with tf.name_scope(\"Encoding_stage\"):\n",
    "        sequence_length = tf.reduce_sum(self.video_mask, 1)\n",
    "        with tf.variable_scope(\"RNN_1\") as scope:            \n",
    "            outputs_encoding,last_state_encoding = tf.nn.dynamic_rnn(\n",
    "                                                        cell=cell_1,\n",
    "                                                        sequence_length=sequence_length,\n",
    "                                                        # inputs=self.image_encoded,\n",
    "                                                        inputs=self.rnn_input,\n",
    "                                                        dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"Decoding_state\"):\n",
    "\n",
    "        sequence_length = tf.reduce_sum(self.caption_mask, 1)\n",
    "        caption_needed = self.caption_encoded[:,:-1,:]\n",
    "\n",
    "        with tf.variable_scope(\"Attention\") as scope:\n",
    "            attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                            num_units = self.attention_num_units, # depth of query mechanism\n",
    "                            memory = outputs_encoding, # hidden states to attend (output of RNN)\n",
    "                            #memory_sequence_length= T,#tf.sequence_mask(seq_lengths, T), # masks false memories\n",
    "                            normalize=False, # normalize energy term\n",
    "                            name='BahdanauAttention')\n",
    "            attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                            cell = cell_2,# Instance of RNNCell\n",
    "                            attention_mechanism = attn_mech, # Instance of AttentionMechanism\n",
    "                            attention_layer_size = self.attention_layer_units, # Int, depth of attention (output) tensor\n",
    "                            alignment_history=True, # whether to store history in final output\n",
    "                            output_attention=False,\n",
    "                            name=\"attention_wrapper\")\n",
    "\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                        inputs = caption_needed, # decoder inputs\n",
    "                        sequence_length = sequence_length, # decoder input length\n",
    "                        name = \"decoder_training_helper\")\n",
    "\n",
    "        attn_zero_state = attn_cell.zero_state(batch_size=batch_size_tensor,dtype=tf.float32)\n",
    "        initial_decoder_state = attn_zero_state.clone(cell_state=last_state_encoding)\n",
    "\n",
    "        with tf.variable_scope(\"RNN_2\") as scope:\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                            cell = attn_cell,\n",
    "                            helper = helper,\n",
    "                            initial_state = initial_decoder_state,\n",
    "                            output_layer = self.rnn_transform_layer)\n",
    "            outputs_decoding, last_state_decoding, output_seq_len = tf.contrib.seq2seq.dynamic_decode(decoder,impute_finished=True)\n",
    "    self.train_output_len = output_seq_len\n",
    "    self.train_output_rnn = outputs_decoding.rnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "last_state_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hist = last_state_decoding[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hh = hist.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
