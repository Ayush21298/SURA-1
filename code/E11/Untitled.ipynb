{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Inference(object):\n",
    "    def __init__ (self,model,word_to_idx,idx_to_word):\n",
    "        self.model=model\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "    def feed_video(self, sess, encoded_video):\n",
    "        initial_state_1,initial_state_2 = sess.run([self.model.infer_hidden_state_1,\n",
    "                                                    self.model.infer_hidden_state_2,],\n",
    "                                                    feed_dict = {self.model.rnn_input: encoded_video, self.model.is_training: False})\n",
    "        return [initial_state_1,initial_state_2]  # (2,3,2,?,1000)\n",
    "    def inference_step(self,sess,input_feed,state_feed):\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.model.state_feed_1] = state_feed[0]  # want (?,6000)\n",
    "        feed_dict[self.model.state_feed_2] = state_feed[1]  # want (?,6000)\n",
    "        feed_dict[self.model.word_input] = input_feed\n",
    "        \n",
    "        preds,next_state_1,next_state_2 = sess.run(\n",
    "                                    [self.model.infer_predictions,\n",
    "                                     self.model.infer_last_state_l1,\n",
    "                                     self.model.infer_last_state_l2],\n",
    "                                     feed_dict=feed_dict)\n",
    "        return preds,[next_state_1,next_state_2]\n",
    "    def generate_caption_batch(self,sess,video_batch,max_len=20):\n",
    "        input_batch = np.array([[self.word_to_idx[\"<bos>\"]]]*video_batch.shape[0])\n",
    "        state = self.feed_video(sess,video_batch)\n",
    "        eos_batch = np.array([[self.word_to_idx[\"<eos>\"]]]*video_batch.shape[0])\n",
    "        finished_batch = np.array([[False]]*video_batch.shape[0])\n",
    "        caption_generated = [\"\" for i in range(video_batch.shape[0])]\n",
    "        loss = [0.0 for d in range(video_batch.shape[0])] \n",
    "        for i in range(max_len):\n",
    "            pred,state = self.inference_step(sess,input_batch,state)\n",
    "            input_batch = np.argmax(pred,axis=2)\n",
    "            pred = np.squeeze(pred,axis=1)            \n",
    "            pred_values = np.max(pred,axis=1)\n",
    "            is_end = np.all(finished_batch)\n",
    "            if is_end:\n",
    "                break\n",
    "            for idx,prob in enumerate(pred):\n",
    "                if not finished_batch[idx]:\n",
    "                    loss[idx] -= np.log(pred_values[idx])\n",
    "                    if self.word_to_idx[\"<eos>\"] != np.argmax(prob):\n",
    "                        caption_generated[idx] += \" \"+self.idx_to_word[np.argmax(prob)]\n",
    "            finished_batch = np.logical_or(input_batch==eos_batch,finished_batch)\n",
    "            \n",
    "        return [[(l/(len(i[1:].split(\" \")) + 1),i[1:])] for l,i in zip(loss,caption_generated)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "from model import Model_S2VT\n",
    "from data_generator import Data_Generator\n",
    "from inference_util import Inference\n",
    "\n",
    "import inception_base\n",
    "import configuration\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                       \"Batch size of train data input.\")\n",
    "tf.flags.DEFINE_integer(\"beam_size\", 3,\n",
    "                       \"Beam size.\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_model\", None,\n",
    "                       \"Model Checkpoint to use.\")\n",
    "tf.flags.DEFINE_integer(\"max_captions\", None,\n",
    "                       \"Maximum number of captions to generate\")\n",
    "tf.flags.DEFINE_integer(\"max_len_captions\", None,\n",
    "                       \"Maximum length of captions to generate\")\n",
    "tf.flags.DEFINE_string(\"dataset\", \"test\",\n",
    "                       \"Dataset to use\")\n",
    "tf.flags.DEFINE_string(\"outfile_name\", \"generated_caption.json\",\n",
    "                       \"Name of the output result file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    FLAGS._parse_flags()\n",
    "    data_config = configuration.DataConfig().config\n",
    "\n",
    "    if FLAGS.checkpoint_model:\n",
    "        model_path = FLAGS.checkpoint_model\n",
    "    else:\n",
    "        model_path = tf.train.latest_checkpoint(data_config[\"checkpoint_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        data_config = configuration.DataConfig().config\n",
    "        data_gen = Data_Generator(processed_video_dir = data_config[\"processed_video_dir\"],\n",
    "                                caption_file = data_config[\"caption_file\"],\n",
    "                                unique_freq_cutoff = data_config[\"unique_frequency_cutoff\"],\n",
    "                                max_caption_len = data_config[\"max_caption_length\"])\n",
    "\n",
    "        data_gen.load_vocabulary(data_config[\"caption_data_dir\"])\n",
    "        data_gen.load_dataset(data_config[\"caption_data_dir\"])\n",
    "\n",
    "        assert FLAGS.dataset in [\"val\",\"test\",\"train\"]\n",
    "\n",
    "        if FLAGS.max_len_captions:\n",
    "            max_len = FLAGS.max_len_captions\n",
    "        else:\n",
    "            max_len = data_config['max_caption_length']\n",
    "\n",
    "        model_config = configuration.ModelConfig(data_gen).config\n",
    "        model = Model_S2VT( num_frames = model_config[\"num_frames\"],\n",
    "                            image_width = model_config[\"image_width\"],\n",
    "                            image_height = model_config[\"image_height\"],\n",
    "                            image_channels = model_config[\"image_channels\"],\n",
    "                            num_caption_unroll = model_config[\"num_caption_unroll\"],\n",
    "                            num_last_layer_units = model_config[\"num_last_layer_units\"],\n",
    "                            image_embedding_size = model_config[\"image_embedding_size\"],\n",
    "                            word_embedding_size = model_config[\"word_embedding_size\"],\n",
    "                            hidden_size_lstm1 = model_config[\"hidden_size_lstm1\"],\n",
    "                            hidden_size_lstm2 = model_config[\"hidden_size_lstm2\"],\n",
    "                            vocab_size = model_config[\"vocab_size\"],\n",
    "                            initializer_scale = model_config[\"initializer_scale\"],\n",
    "                            learning_rate = model_config[\"learning_rate\"],\n",
    "                            mode=\"inference\",\n",
    "                            rnn1_input_keep_prob=model_config[\"rnn1_input_keep_prob\"],\n",
    "                            rnn1_output_keep_prob=model_config[\"rnn1_output_keep_prob\"],\n",
    "                            rnn2_input_keep_prob=model_config[\"rnn2_input_keep_prob\"],\n",
    "                            rnn2_output_keep_prob=model_config[\"rnn2_output_keep_prob\"],\n",
    "                            num_layers_per_rnn=model_config[\"num_layers_per_rnn\"]\n",
    "                            )\n",
    "        model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infer_util = Inference(model,data_gen.word_to_idx,data_gen.idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self = infer_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if FLAGS.max_captions:\n",
    "            max_iter = FLAGS.max_captions\n",
    "        else:\n",
    "            max_iter = len(data_gen.dataset[FLAGS.dataset])+10 #+10 is just to be safe ;)\n",
    "        \n",
    "        video_paths = {i[\"file_name\"]:i[\"path\"] for i in data_gen.dataset[FLAGS.dataset]}\n",
    "\n",
    "        \n",
    "        gen_captions = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = tf.train.latest_checkpoint(data_config[\"checkpoint_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beam_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_batch = dataset[\"video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "        assert beam_size >= 2\n",
    "        input_batch = np.array([[self.word_to_idx[\"<bos>\"]]]*video_batch.shape[0])\n",
    "        # state = tf.convert_to_tensor(np.asarray(self.feed_video(sess,video_batch)))  # (2,3,2,?,1000)\n",
    "        state = np.asarray(self.feed_video(sess,video_batch))  # (2,3,2,?,1000)\n",
    "        batch_of_beams = []\n",
    "        for i in range(video_batch.shape[0]):\n",
    "            beam = [] # {st: , current_cap: , loss: , prev_word:}\n",
    "            #for j in range(beam_size):\n",
    "            beam.append({    \"st\":  state[:,:,:,i,:],                      #[state[k][i] for k in range(len(state))],\n",
    "                             \"current_cap\":\"\" ,\n",
    "                             \"loss\":0,\n",
    "                             \"prev_word\":self.word_to_idx[\"<bos>\"] })\n",
    "            batch_of_beams.append(beam)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'current_cap': '',\n",
       "  'loss': 0,\n",
       "  'prev_word': 1,\n",
       "  'st': array([[[[ -9.99838333e+01,   2.04102921e+00,   9.14099274e+01, ...,\n",
       "             -1.59060247e-02,  -9.60721054e+01,  -7.53348589e+00],\n",
       "           [ -2.09867994e-05,   6.77062869e-01,   2.60653610e-06, ...,\n",
       "             -8.73918623e-07,  -1.94410095e-04,  -1.73635719e-08]],\n",
       "  \n",
       "          [[  9.88782346e-01,  -2.07560749e+01,   7.47688770e-01, ...,\n",
       "             -1.04166579e+00,   2.61445105e-01,   1.73813291e-02],\n",
       "           [  8.42868462e-02,  -4.74841446e-02,   3.69045347e-01, ...,\n",
       "             -6.29446447e-01,   1.42354354e-01,   1.72829512e-03]],\n",
       "  \n",
       "          [[ -7.28070021e-01,  -1.00246280e-01,  -4.21963283e-04, ...,\n",
       "             -7.55462980e+00,   4.20989394e-01,   7.29404545e+00],\n",
       "           [ -3.50736737e-01,  -1.82715780e-03,  -8.16863394e-05, ...,\n",
       "             -5.00280440e-01,   1.39130965e-01,   2.68254399e-01]]],\n",
       "  \n",
       "  \n",
       "         [[[ -9.73954010e+01,  -8.10542755e+01,   9.58131409e+01, ...,\n",
       "              6.73773575e+01,   9.24972076e+01,  -9.67322388e+01],\n",
       "           [ -1.74283847e-01,  -2.67621642e-03,   2.13581137e-03, ...,\n",
       "              1.15813171e-04,   2.06356941e-04,  -4.57189791e-02]],\n",
       "  \n",
       "          [[ -6.19329567e+01,  -1.32430232e+00,  -1.24960470e+00, ...,\n",
       "             -3.42204928e-01,  -1.47594586e-01,  -1.50005400e-01],\n",
       "           [ -2.41855727e-04,  -8.58279049e-01,  -2.86699757e-02, ...,\n",
       "             -1.40023732e-03,  -1.26669798e-02,  -1.10758699e-01]],\n",
       "  \n",
       "          [[  1.00180542e+00,   3.42435747e-01,   9.64598656e-01, ...,\n",
       "             -1.31947547e-03,   1.12709418e-01,  -6.22775733e-01],\n",
       "           [  7.16221452e-01,   2.79956371e-01,   8.31596851e-02, ...,\n",
       "             -8.53488571e-04,   3.57186794e-02,  -2.80574322e-01]]]], dtype=float32)}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2, 1000)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_beams[0][0][\"st\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -9.99838333e+01,   2.04102921e+00,   9.14099274e+01, ...,\n",
       "           -1.59060247e-02,  -9.60721054e+01,  -7.53348589e+00],\n",
       "         [ -2.09867994e-05,   6.77062869e-01,   2.60653610e-06, ...,\n",
       "           -8.73918623e-07,  -1.94410095e-04,  -1.73635719e-08]],\n",
       "\n",
       "        [[  9.88782346e-01,  -2.07560749e+01,   7.47688770e-01, ...,\n",
       "           -1.04166579e+00,   2.61445105e-01,   1.73813291e-02],\n",
       "         [  8.42868462e-02,  -4.74841446e-02,   3.69045347e-01, ...,\n",
       "           -6.29446447e-01,   1.42354354e-01,   1.72829512e-03]],\n",
       "\n",
       "        [[ -7.28070021e-01,  -1.00246280e-01,  -4.21963283e-04, ...,\n",
       "           -7.55462980e+00,   4.20989394e-01,   7.29404545e+00],\n",
       "         [ -3.50736737e-01,  -1.82715780e-03,  -8.16863394e-05, ...,\n",
       "           -5.00280440e-01,   1.39130965e-01,   2.68254399e-01]]],\n",
       "\n",
       "\n",
       "       [[[ -9.73954010e+01,  -8.10542755e+01,   9.58131409e+01, ...,\n",
       "            6.73773575e+01,   9.24972076e+01,  -9.67322388e+01],\n",
       "         [ -1.74283847e-01,  -2.67621642e-03,   2.13581137e-03, ...,\n",
       "            1.15813171e-04,   2.06356941e-04,  -4.57189791e-02]],\n",
       "\n",
       "        [[ -6.19329567e+01,  -1.32430232e+00,  -1.24960470e+00, ...,\n",
       "           -3.42204928e-01,  -1.47594586e-01,  -1.50005400e-01],\n",
       "         [ -2.41855727e-04,  -8.58279049e-01,  -2.86699757e-02, ...,\n",
       "           -1.40023732e-03,  -1.26669798e-02,  -1.10758699e-01]],\n",
       "\n",
       "        [[  1.00180542e+00,   3.42435747e-01,   9.64598656e-01, ...,\n",
       "           -1.31947547e-03,   1.12709418e-01,  -6.22775733e-01],\n",
       "         [  7.16221452e-01,   2.79956371e-01,   8.31596851e-02, ...,\n",
       "           -8.53488571e-04,   3.57186794e-02,  -2.80574322e-01]]]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_of_beams[0][0][\"st\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-c993f993089e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch_of_beams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"st\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "batch_of_beams[0][1][\"st\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        completed_captions = [[] for d in range(video_batch.shape[0])] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beam_squared_list = [[] for d in range(video_batch.shape[0])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_of_beams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vv = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                input_batch = [[video[vv][\"prev_word\"]] for video in batch_of_beams]\n",
    "\n",
    "                state = np.array([video[vv][\"st\"] for video in batch_of_beams]) #? * 2 * 3 * 2 * 1000\n",
    "                # state = tf.reshape(state,[-1,2,6000])\n",
    "                state = np.reshape(state,[-1,2,6000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 2, 6000)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.18862305e+01,   1.45790863e+00,   2.56467018e+01, ...,\n",
       "         -1.32005705e-04,  -5.85127389e-03,   1.77215447e-03],\n",
       "       [ -9.39445190e+01,  -7.88581924e+01,   9.63216171e+01, ...,\n",
       "         -4.79316682e-01,   2.35233217e-01,  -1.36554256e-01]], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -8.10724106e+01,  -6.73338711e-01,   3.68155251e+01, ...,\n",
       "         -1.21881385e-05,   4.77400608e-04,   5.91508928e-04],\n",
       "       [ -9.08534393e+01,  -6.09099655e+01,   8.04283676e+01, ...,\n",
       "         -2.66367465e-01,   1.11403890e-01,  -1.10604897e-01]], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                pred,state = self.inference_step(sess,input_batch,state.swapaxes(0,1))\n",
    "                state = np.asarray(state)\n",
    "                pred = np.squeeze(pred,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2, 64, 1000)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -6.05223579e+01,   6.38768196e-01,   6.17066526e+00, ...,\n",
       "            4.06917892e-02,  -5.38499298e+01,  -3.69563401e-01],\n",
       "         [ -1.02895215e-01,   2.15328764e-02,   4.47077379e-02, ...,\n",
       "            2.97516887e-03,  -4.58392911e-02,  -5.94801679e-02]],\n",
       "\n",
       "        [[ -3.54041278e-01,  -1.81968384e+01,  -4.81483251e-01, ...,\n",
       "           -5.93857393e-02,   6.79109514e-01,   9.84028056e-02],\n",
       "         [ -5.06397849e-03,  -8.08976352e-01,  -3.72972488e-02, ...,\n",
       "           -4.57791612e-04,   4.84963227e-03,   1.45565733e-04]],\n",
       "\n",
       "        [[ -3.96058476e-03,  -9.68905449e-01,   1.78015127e-03, ...,\n",
       "           -8.49343777e+00,  -4.11529876e-02,   2.66327858e+00],\n",
       "         [ -1.43273239e-04,  -8.33399186e-04,   4.88800936e-07, ...,\n",
       "           -8.27363328e-05,  -5.19499707e-04,   1.25170627e-03]]],\n",
       "\n",
       "\n",
       "       [[[ -8.24110107e+01,  -7.19346161e+01,   8.73680801e+01, ...,\n",
       "            6.69161682e+01,   9.16233673e+01,  -7.74344482e+01],\n",
       "         [ -6.48962334e-02,  -1.22694662e-02,   4.32256460e-02, ...,\n",
       "            7.77958035e-02,   5.91856316e-02,  -4.52972174e-01]],\n",
       "\n",
       "        [[ -3.58042564e+01,  -9.89907265e-01,  -1.85825288e-01, ...,\n",
       "            1.47157311e-01,  -9.31272030e-01,  -1.28085005e+00],\n",
       "         [ -9.89236772e-01,  -7.57223547e-01,  -6.68383483e-03, ...,\n",
       "            1.46076098e-01,  -1.26953959e-01,  -8.46455634e-01]],\n",
       "\n",
       "        [[  9.46178511e-02,   3.12409639e-01,  -7.31832922e-01, ...,\n",
       "            4.19411361e-01,   2.09664926e-02,  -6.42079294e-01],\n",
       "         [  5.40528893e-02,   1.65613711e-01,  -5.87503910e-01, ...,\n",
       "            3.16323012e-01,   1.31868618e-02,  -3.62219512e-02]]]], dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[:,:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ -6.68741379e+01,  -5.03829122e-01,   1.05510778e+01, ...,\n",
       "           -4.65097837e-03,  -5.82719383e+01,  -9.94058251e-02],\n",
       "         [ -6.77893236e-02,  -1.94962993e-02,   2.37251241e-02, ...,\n",
       "           -3.21857515e-04,  -4.78835516e-02,  -1.93420984e-02]],\n",
       "\n",
       "        [[ -8.18027306e+00,  -2.85391521e+00,  -1.67784795e-01, ...,\n",
       "           -7.98211247e-03,   4.77696925e-01,   1.53137967e-01],\n",
       "         [ -2.39530881e-03,  -5.86891174e-01,  -3.01803686e-02, ...,\n",
       "           -1.18742784e-04,   5.03763149e-04,   2.78546304e-05]],\n",
       "\n",
       "        [[ -3.75485932e-03,  -9.80784655e-01,   4.87186573e-03, ...,\n",
       "           -6.30064917e+00,  -1.13337822e-01,   1.70398402e+00],\n",
       "         [ -1.86717516e-04,  -1.45497639e-03,   3.12047541e-06, ...,\n",
       "           -1.36619099e-04,  -1.81789312e-03,   1.89936673e-03]]],\n",
       "\n",
       "\n",
       "       [[[ -8.26215134e+01,  -5.77710609e+01,   7.32980042e+01, ...,\n",
       "            6.40386810e+01,   7.31793594e+01,  -4.03797722e+01],\n",
       "         [ -5.71778193e-02,  -1.23174358e-02,   3.43081951e-02, ...,\n",
       "            2.80494727e-02,   1.32571608e-02,  -7.45829046e-01]],\n",
       "\n",
       "        [[ -2.68995705e+01,  -5.74701548e-01,  -8.10675621e+00, ...,\n",
       "            8.20999146e-02,  -1.17853022e+00,  -9.74103093e-01],\n",
       "         [ -9.80511725e-01,  -5.18744230e-01,  -6.09371020e-03, ...,\n",
       "            8.18625838e-02,  -6.70616236e-03,  -7.25697517e-01]],\n",
       "\n",
       "        [[  5.46117350e-02,  -1.96870542e+00,  -4.73839879e-01, ...,\n",
       "            2.02751398e-01,   1.65629372e-01,  -8.91174793e-01],\n",
       "         [  2.70063821e-02,  -2.09093373e-02,  -3.52748781e-01, ...,\n",
       "            1.21246502e-01,   9.14974436e-02,  -1.12082012e-01]]]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[:,:,:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                for j in range(len(beam_squared_list)):\n",
    "                    #print(\"-------- Vid ------\",j, \" Current cap \", batch_of_beams[j][vv][\"current_cap\"] )\n",
    "                    for pred_word in pred[j].argsort()[-beam_size:][::-1]:\n",
    "                        #print(pred_word, \" Pred Word-- \", self.idx_to_word[pred_word])\n",
    "                        new_loss = batch_of_beams[j][vv][\"loss\"] - np.log(pred[j][pred_word])\n",
    "                        if (pred_word == self.word_to_idx[\"<eos>\"]) : \n",
    "                            completed_captions[j].append(( new_loss / (i+1) , # did  +1 avoiding divide by 0\n",
    "                                                          batch_of_beams[j][vv][\"current_cap\"]))\n",
    "                        else:\n",
    "                            beam_squared_list[j].append({\"st\":  state[:,:,:,j,:], #,[state[k][j] for k in range(len(state))] ,\n",
    "                                                         \"current_cap\":batch_of_beams[j][vv][\"current_cap\"] + \n",
    "                                                            \" \" + self.idx_to_word[pred_word],\n",
    "                                                         \"loss\":new_loss,\n",
    "                                                         \"prev_word\":pred_word })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'current_cap': ' a woman is',\n",
       "  'loss': 1.4964427649974823,\n",
       "  'prev_word': 5,\n",
       "  'st': array([[[[ -6.05223579e+01,   6.38768196e-01,   6.17066526e+00, ...,\n",
       "              4.06917892e-02,  -5.38499298e+01,  -3.69563401e-01],\n",
       "           [ -1.02895215e-01,   2.15328764e-02,   4.47077379e-02, ...,\n",
       "              2.97516887e-03,  -4.58392911e-02,  -5.94801679e-02]],\n",
       "  \n",
       "          [[ -3.54041278e-01,  -1.81968384e+01,  -4.81483251e-01, ...,\n",
       "             -5.93857393e-02,   6.79109514e-01,   9.84028056e-02],\n",
       "           [ -5.06397849e-03,  -8.08976352e-01,  -3.72972488e-02, ...,\n",
       "             -4.57791612e-04,   4.84963227e-03,   1.45565733e-04]],\n",
       "  \n",
       "          [[ -3.96058476e-03,  -9.68905449e-01,   1.78015127e-03, ...,\n",
       "             -8.49343777e+00,  -4.11529876e-02,   2.66327858e+00],\n",
       "           [ -1.43273239e-04,  -8.33399186e-04,   4.88800936e-07, ...,\n",
       "             -8.27363328e-05,  -5.19499707e-04,   1.25170627e-03]]],\n",
       "  \n",
       "  \n",
       "         [[[ -8.24110107e+01,  -7.19346161e+01,   8.73680801e+01, ...,\n",
       "              6.69161682e+01,   9.16233673e+01,  -7.74344482e+01],\n",
       "           [ -6.48962334e-02,  -1.22694662e-02,   4.32256460e-02, ...,\n",
       "              7.77958035e-02,   5.91856316e-02,  -4.52972174e-01]],\n",
       "  \n",
       "          [[ -3.58042564e+01,  -9.89907265e-01,  -1.85825288e-01, ...,\n",
       "              1.47157311e-01,  -9.31272030e-01,  -1.28085005e+00],\n",
       "           [ -9.89236772e-01,  -7.57223547e-01,  -6.68383483e-03, ...,\n",
       "              1.46076098e-01,  -1.26953959e-01,  -8.46455634e-01]],\n",
       "  \n",
       "          [[  9.46178511e-02,   3.12409639e-01,  -7.31832922e-01, ...,\n",
       "              4.19411361e-01,   2.09664926e-02,  -6.42079294e-01],\n",
       "           [  5.40528893e-02,   1.65613711e-01,  -5.87503910e-01, ...,\n",
       "              3.16323012e-01,   1.31868618e-02,  -3.62219512e-02]]]], dtype=float32)},\n",
       " {'current_cap': ' a woman talks',\n",
       "  'loss': 3.3558920919895172,\n",
       "  'prev_word': 45,\n",
       "  'st': array([[[[ -6.05223579e+01,   6.38768196e-01,   6.17066526e+00, ...,\n",
       "              4.06917892e-02,  -5.38499298e+01,  -3.69563401e-01],\n",
       "           [ -1.02895215e-01,   2.15328764e-02,   4.47077379e-02, ...,\n",
       "              2.97516887e-03,  -4.58392911e-02,  -5.94801679e-02]],\n",
       "  \n",
       "          [[ -3.54041278e-01,  -1.81968384e+01,  -4.81483251e-01, ...,\n",
       "             -5.93857393e-02,   6.79109514e-01,   9.84028056e-02],\n",
       "           [ -5.06397849e-03,  -8.08976352e-01,  -3.72972488e-02, ...,\n",
       "             -4.57791612e-04,   4.84963227e-03,   1.45565733e-04]],\n",
       "  \n",
       "          [[ -3.96058476e-03,  -9.68905449e-01,   1.78015127e-03, ...,\n",
       "             -8.49343777e+00,  -4.11529876e-02,   2.66327858e+00],\n",
       "           [ -1.43273239e-04,  -8.33399186e-04,   4.88800936e-07, ...,\n",
       "             -8.27363328e-05,  -5.19499707e-04,   1.25170627e-03]]],\n",
       "  \n",
       "  \n",
       "         [[[ -8.24110107e+01,  -7.19346161e+01,   8.73680801e+01, ...,\n",
       "              6.69161682e+01,   9.16233673e+01,  -7.74344482e+01],\n",
       "           [ -6.48962334e-02,  -1.22694662e-02,   4.32256460e-02, ...,\n",
       "              7.77958035e-02,   5.91856316e-02,  -4.52972174e-01]],\n",
       "  \n",
       "          [[ -3.58042564e+01,  -9.89907265e-01,  -1.85825288e-01, ...,\n",
       "              1.47157311e-01,  -9.31272030e-01,  -1.28085005e+00],\n",
       "           [ -9.89236772e-01,  -7.57223547e-01,  -6.68383483e-03, ...,\n",
       "              1.46076098e-01,  -1.26953959e-01,  -8.46455634e-01]],\n",
       "  \n",
       "          [[  9.46178511e-02,   3.12409639e-01,  -7.31832922e-01, ...,\n",
       "              4.19411361e-01,   2.09664926e-02,  -6.42079294e-01],\n",
       "           [  5.40528893e-02,   1.65613711e-01,  -5.87503910e-01, ...,\n",
       "              3.16323012e-01,   1.31868618e-02,  -3.62219512e-02]]]], dtype=float32)},\n",
       " {'current_cap': ' a woman talking',\n",
       "  'loss': 3.4565571844577789,\n",
       "  'prev_word': 17,\n",
       "  'st': array([[[[ -6.05223579e+01,   6.38768196e-01,   6.17066526e+00, ...,\n",
       "              4.06917892e-02,  -5.38499298e+01,  -3.69563401e-01],\n",
       "           [ -1.02895215e-01,   2.15328764e-02,   4.47077379e-02, ...,\n",
       "              2.97516887e-03,  -4.58392911e-02,  -5.94801679e-02]],\n",
       "  \n",
       "          [[ -3.54041278e-01,  -1.81968384e+01,  -4.81483251e-01, ...,\n",
       "             -5.93857393e-02,   6.79109514e-01,   9.84028056e-02],\n",
       "           [ -5.06397849e-03,  -8.08976352e-01,  -3.72972488e-02, ...,\n",
       "             -4.57791612e-04,   4.84963227e-03,   1.45565733e-04]],\n",
       "  \n",
       "          [[ -3.96058476e-03,  -9.68905449e-01,   1.78015127e-03, ...,\n",
       "             -8.49343777e+00,  -4.11529876e-02,   2.66327858e+00],\n",
       "           [ -1.43273239e-04,  -8.33399186e-04,   4.88800936e-07, ...,\n",
       "             -8.27363328e-05,  -5.19499707e-04,   1.25170627e-03]]],\n",
       "  \n",
       "  \n",
       "         [[[ -8.24110107e+01,  -7.19346161e+01,   8.73680801e+01, ...,\n",
       "              6.69161682e+01,   9.16233673e+01,  -7.74344482e+01],\n",
       "           [ -6.48962334e-02,  -1.22694662e-02,   4.32256460e-02, ...,\n",
       "              7.77958035e-02,   5.91856316e-02,  -4.52972174e-01]],\n",
       "  \n",
       "          [[ -3.58042564e+01,  -9.89907265e-01,  -1.85825288e-01, ...,\n",
       "              1.47157311e-01,  -9.31272030e-01,  -1.28085005e+00],\n",
       "           [ -9.89236772e-01,  -7.57223547e-01,  -6.68383483e-03, ...,\n",
       "              1.46076098e-01,  -1.26953959e-01,  -8.46455634e-01]],\n",
       "  \n",
       "          [[  9.46178511e-02,   3.12409639e-01,  -7.31832922e-01, ...,\n",
       "              4.19411361e-01,   2.09664926e-02,  -6.42079294e-01],\n",
       "           [  5.40528893e-02,   1.65613711e-01,  -5.87503910e-01, ...,\n",
       "              3.16323012e-01,   1.31868618e-02,  -3.62219512e-02]]]], dtype=float32)}]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_squared_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'current_cap': ' a man is',\n",
       "  'loss': 2.342376172542572,\n",
       "  'prev_word': 5,\n",
       "  'st': array([[[[ -6.03195152e+01,  -1.38572991e-01,   1.32158136e+01, ...,\n",
       "              3.01691115e-01,  -4.20910454e+01,   1.76077560e-02],\n",
       "           [ -1.91515610e-01,  -3.95993423e-03,   1.10232875e-01, ...,\n",
       "              2.02649347e-02,  -4.91376854e-02,   4.31244168e-03]],\n",
       "  \n",
       "          [[ -2.22946143e+00,  -6.13076627e-01,  -4.72688884e-01, ...,\n",
       "             -6.34076297e-02,   1.06641412e+00,   6.49412572e-01],\n",
       "           [ -5.07369228e-02,  -4.66130108e-01,  -2.73120198e-02, ...,\n",
       "             -2.14057349e-04,   2.27711890e-02,   1.09344088e-02]],\n",
       "  \n",
       "          [[ -1.46801560e-03,  -9.89146888e-01,   5.92224009e-04, ...,\n",
       "             -1.72135563e+01,  -2.05453053e-01,   2.26917577e+00],\n",
       "           [ -3.00105476e-05,  -4.49942017e-04,   3.26415588e-08, ...,\n",
       "             -2.22821709e-05,  -3.39500536e-03,   3.24071734e-04]]],\n",
       "  \n",
       "  \n",
       "         [[[ -8.20154648e+01,  -1.35776825e+01,   6.87278061e+01, ...,\n",
       "              1.91134620e+00,   3.84326134e+01,  -6.01801109e+00],\n",
       "           [ -5.79524562e-02,  -1.03706084e-02,   2.89724953e-02, ...,\n",
       "              2.92391833e-02,   1.43740512e-02,  -8.77903223e-01]],\n",
       "  \n",
       "          [[ -5.28279839e+01,  -9.65330899e-01,  -7.26719558e-01, ...,\n",
       "             -5.53933024e-01,  -1.68332672e+00,   5.69132614e+00],\n",
       "           [ -9.81541514e-01,  -7.46510625e-01,  -4.91744094e-03, ...,\n",
       "             -5.02926588e-01,  -1.14919759e-01,   9.90583718e-01]],\n",
       "  \n",
       "          [[ -1.07908082e+00,  -2.30229721e+01,  -8.11840892e-01, ...,\n",
       "              1.94245949e-02,   3.51966977e-01,  -1.11871910e+00],\n",
       "           [ -1.79669201e-01,  -2.67864428e-02,  -6.39800847e-01, ...,\n",
       "              1.89006180e-02,   2.04897121e-01,  -3.66976336e-02]]]], dtype=float32)},\n",
       " {'current_cap': ' a man playing',\n",
       "  'loss': 3.7916808128356934,\n",
       "  'prev_word': 21,\n",
       "  'st': array([[[[ -6.03195152e+01,  -1.38572991e-01,   1.32158136e+01, ...,\n",
       "              3.01691115e-01,  -4.20910454e+01,   1.76077560e-02],\n",
       "           [ -1.91515610e-01,  -3.95993423e-03,   1.10232875e-01, ...,\n",
       "              2.02649347e-02,  -4.91376854e-02,   4.31244168e-03]],\n",
       "  \n",
       "          [[ -2.22946143e+00,  -6.13076627e-01,  -4.72688884e-01, ...,\n",
       "             -6.34076297e-02,   1.06641412e+00,   6.49412572e-01],\n",
       "           [ -5.07369228e-02,  -4.66130108e-01,  -2.73120198e-02, ...,\n",
       "             -2.14057349e-04,   2.27711890e-02,   1.09344088e-02]],\n",
       "  \n",
       "          [[ -1.46801560e-03,  -9.89146888e-01,   5.92224009e-04, ...,\n",
       "             -1.72135563e+01,  -2.05453053e-01,   2.26917577e+00],\n",
       "           [ -3.00105476e-05,  -4.49942017e-04,   3.26415588e-08, ...,\n",
       "             -2.22821709e-05,  -3.39500536e-03,   3.24071734e-04]]],\n",
       "  \n",
       "  \n",
       "         [[[ -8.20154648e+01,  -1.35776825e+01,   6.87278061e+01, ...,\n",
       "              1.91134620e+00,   3.84326134e+01,  -6.01801109e+00],\n",
       "           [ -5.79524562e-02,  -1.03706084e-02,   2.89724953e-02, ...,\n",
       "              2.92391833e-02,   1.43740512e-02,  -8.77903223e-01]],\n",
       "  \n",
       "          [[ -5.28279839e+01,  -9.65330899e-01,  -7.26719558e-01, ...,\n",
       "             -5.53933024e-01,  -1.68332672e+00,   5.69132614e+00],\n",
       "           [ -9.81541514e-01,  -7.46510625e-01,  -4.91744094e-03, ...,\n",
       "             -5.02926588e-01,  -1.14919759e-01,   9.90583718e-01]],\n",
       "  \n",
       "          [[ -1.07908082e+00,  -2.30229721e+01,  -8.11840892e-01, ...,\n",
       "              1.94245949e-02,   3.51966977e-01,  -1.11871910e+00],\n",
       "           [ -1.79669201e-01,  -2.67864428e-02,  -6.39800847e-01, ...,\n",
       "              1.89006180e-02,   2.04897121e-01,  -3.66976336e-02]]]], dtype=float32)},\n",
       " {'current_cap': ' a man plays',\n",
       "  'loss': 4.4292702674865723,\n",
       "  'prev_word': 89,\n",
       "  'st': array([[[[ -6.03195152e+01,  -1.38572991e-01,   1.32158136e+01, ...,\n",
       "              3.01691115e-01,  -4.20910454e+01,   1.76077560e-02],\n",
       "           [ -1.91515610e-01,  -3.95993423e-03,   1.10232875e-01, ...,\n",
       "              2.02649347e-02,  -4.91376854e-02,   4.31244168e-03]],\n",
       "  \n",
       "          [[ -2.22946143e+00,  -6.13076627e-01,  -4.72688884e-01, ...,\n",
       "             -6.34076297e-02,   1.06641412e+00,   6.49412572e-01],\n",
       "           [ -5.07369228e-02,  -4.66130108e-01,  -2.73120198e-02, ...,\n",
       "             -2.14057349e-04,   2.27711890e-02,   1.09344088e-02]],\n",
       "  \n",
       "          [[ -1.46801560e-03,  -9.89146888e-01,   5.92224009e-04, ...,\n",
       "             -1.72135563e+01,  -2.05453053e-01,   2.26917577e+00],\n",
       "           [ -3.00105476e-05,  -4.49942017e-04,   3.26415588e-08, ...,\n",
       "             -2.22821709e-05,  -3.39500536e-03,   3.24071734e-04]]],\n",
       "  \n",
       "  \n",
       "         [[[ -8.20154648e+01,  -1.35776825e+01,   6.87278061e+01, ...,\n",
       "              1.91134620e+00,   3.84326134e+01,  -6.01801109e+00],\n",
       "           [ -5.79524562e-02,  -1.03706084e-02,   2.89724953e-02, ...,\n",
       "              2.92391833e-02,   1.43740512e-02,  -8.77903223e-01]],\n",
       "  \n",
       "          [[ -5.28279839e+01,  -9.65330899e-01,  -7.26719558e-01, ...,\n",
       "             -5.53933024e-01,  -1.68332672e+00,   5.69132614e+00],\n",
       "           [ -9.81541514e-01,  -7.46510625e-01,  -4.91744094e-03, ...,\n",
       "             -5.02926588e-01,  -1.14919759e-01,   9.90583718e-01]],\n",
       "  \n",
       "          [[ -1.07908082e+00,  -2.30229721e+01,  -8.11840892e-01, ...,\n",
       "              1.94245949e-02,   3.51966977e-01,  -1.11871910e+00],\n",
       "           [ -1.79669201e-01,  -2.67864428e-02,  -6.39800847e-01, ...,\n",
       "              1.89006180e-02,   2.04897121e-01,  -3.66976336e-02]]]], dtype=float32)}]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_squared_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            for j in range(len(beam_squared_list)):\n",
    "                beam_squared_list[j].sort(key = lambda x: x[\"loss\"])\n",
    "                batch_of_beams[j] = beam_squared_list[j][:beam_size].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beam_squared_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(state[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state[:,:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = tf.convert_to_tensor(np.asarray(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = sss[:,:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring weights from ./models/train\\model-30540\n",
      "INFO:tensorflow:Restoring parameters from ./models/train\\model-30540\n"
     ]
    }
   ],
   "source": [
    "gen_caption = []    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if model_path != None:\n",
    "    print(\"Restoring weights from %s\" %model_path)\n",
    "    saver.restore(sess,model_path)\n",
    "\n",
    "else:\n",
    "    print(\"No checkpoint found. Exiting\")\n",
    "\n",
    "video_files = list(video_paths.keys())    \n",
    "\n",
    "iter = 0\n",
    "btch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1\n"
     ]
    }
   ],
   "source": [
    "        print(\"Processing batch %d\" %(int(0/batch_size)+1))\n",
    "        start = 0\n",
    "        end = min(len(video_files),0+batch_size)\n",
    "        dataset={}\n",
    "        dataset[\"video\"] = np.asarray([np.load(video_paths[video_files[i]]) for i in range(start,end)])\n",
    "        dataset[\"path\"] = [video_paths[video_files[i]] for i in range(start,end)]\n",
    "        dataset[\"file\"] = [video_files[i] for i in range(start,end)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beam_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_batch  = dataset[\"video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        assert beam_size >= 2\n",
    "        input_batch = np.array([[self.word_to_idx[\"<bos>\"]]]*dataset[\"video\"].shape[0])\n",
    "        state = self.feed_video(sess,dataset[\"video\"])\n",
    "\n",
    "        batch_of_beams = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config[\"num_layers_per_rnn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_hidden_state_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cell_1.state_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_padding = tf.zeros([tf.shape(model.word_encoded)[0],1,model.num_last_layer_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                state_tuple_1 = tf.split(value=model.state_feed_1,\n",
    "                                    num_or_size_splits=model.num_layers_per_rnn, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                state_tuple_1 = tuple([tf.contrib.rnn.LSTMStateTuple(*tf.split(\n",
    "                                value=LayerTuple,\n",
    "                                num_or_size_splits=2, axis=1)\n",
    "                                ) for LayerTuple in state_tuple_1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "                    outputs_l11, last_state_l11 = tf.nn.dynamic_rnn(\n",
    "                                                cell=tf.contrib.rnn.MultiRNNCell(\n",
    "                    [tf.contrib.rnn.BasicLSTMCell(\n",
    "                    model.hidden_size_lstm1,\n",
    "                    reuse=tf.get_variable_scope().reuse) for _ in range(model.num_layers_per_rnn)]),\n",
    "                                                inputs=image_padding,\n",
    "                                                initial_state=state_tuple_1,\n",
    "                                                dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state_l11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_hidden_state_1 = tf.concat(last_state_l11,axis=1,name=\"hidden_state_1_concat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_last_state_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state_l11[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_hidden_state_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infer_hidden_state_111 = tf.concat(last_state_l11,axis=2,name=\"hidden_state_1_concat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_hidden_state_111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
