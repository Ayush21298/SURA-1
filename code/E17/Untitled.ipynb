{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "from model import Caption_Model\n",
    "from data_generator import Data_Generator\n",
    "from inference_util import Inference\n",
    "\n",
    "import inception_base\n",
    "import configuration\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                       \"Batch size of train data input.\")\n",
    "tf.flags.DEFINE_integer(\"beam_size\", 3,\n",
    "                       \"Beam size.\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_model\", None,\n",
    "                       \"Model Checkpoint to use.\")\n",
    "tf.flags.DEFINE_integer(\"max_captions\", None,\n",
    "                       \"Maximum number of captions to generate\")\n",
    "tf.flags.DEFINE_integer(\"max_len_captions\", None,\n",
    "                       \"Maximum length of captions to generate\")\n",
    "tf.flags.DEFINE_string(\"dataset\", \"test\",\n",
    "                       \"Dataset to use\")\n",
    "tf.flags.DEFINE_string(\"outfile_name\", \"generated_caption.json\",\n",
    "                       \"Name of the output result file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "beam_size = 3\n",
    "checkpoint_model = \"./models/train/model-21160\"\n",
    "max_captions = None\n",
    "max_len_captions = 30\n",
    "dataset = \"test\"\n",
    "outfile_name = \"generated_caption.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_config = configuration.DataConfig().config\n",
    "data_gen = Data_Generator(processed_video_dir = data_config[\"processed_video_dir\"],\n",
    "                        caption_file = data_config[\"caption_file\"],\n",
    "                        unique_freq_cutoff = data_config[\"unique_frequency_cutoff\"],\n",
    "                        max_caption_len = data_config[\"max_caption_length\"])\n",
    "\n",
    "data_gen.load_vocabulary(data_config[\"caption_data_dir\"])\n",
    "data_gen.load_dataset(data_config[\"caption_data_dir\"])\n",
    "\n",
    "assert dataset in [\"val\",\"test\",\"train\"]\n",
    "\n",
    "if max_len_captions:\n",
    "    max_len = max_len_captions\n",
    "else:\n",
    "    max_len = data_config['max_caption_length']\n",
    "\n",
    "model_config = configuration.ModelConfig(data_gen).config\n",
    "model = Caption_Model( num_frames = model_config[\"num_frames\"],\n",
    "                    image_width = model_config[\"image_width\"],\n",
    "                    image_height = model_config[\"image_height\"],\n",
    "                    image_channels = model_config[\"image_channels\"],\n",
    "                    num_caption_unroll = model_config[\"num_caption_unroll\"],\n",
    "                    num_last_layer_units = model_config[\"num_last_layer_units\"],\n",
    "                    image_embedding_size = model_config[\"image_embedding_size\"],\n",
    "                    word_embedding_size = model_config[\"word_embedding_size\"],\n",
    "                    hidden_size_lstm = model_config[\"hidden_size_lstm\"],\n",
    "                    num_lstm_layer = model_config[\"num_lstm_layer\"],\n",
    "                    vocab_size = model_config[\"vocab_size\"],\n",
    "                    initializer_scale = model_config[\"initializer_scale\"],\n",
    "                    learning_rate = model_config[\"learning_rate\"],\n",
    "                    mode=\"inference\",\n",
    "                    rnn1_input_keep_prob=model_config[\"rnn1_input_keep_prob\"],\n",
    "                    rnn1_output_keep_prob=model_config[\"rnn1_output_keep_prob\"],\n",
    "                    rnn2_input_keep_prob=model_config[\"rnn2_input_keep_prob\"],\n",
    "                    rnn2_output_keep_prob=model_config[\"rnn2_output_keep_prob\"]\n",
    "                    )\n",
    "model.build()\n",
    "\n",
    "infer_util = Inference(model,data_gen.word_to_idx,data_gen.idx_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if max_captions:\n",
    "    max_iter = max_captions\n",
    "else:\n",
    "    max_iter = len(data_gen.dataset[dataset])+10 #+10 is just to be safe ;)\n",
    "\n",
    "video_paths = {i[\"file_name\"]:i[\"path\"] for i in data_gen.dataset[dataset]}\n",
    "\n",
    "\n",
    "gen_captions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./models/train/model-21160\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gen_caption = []    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if model_path != None:\n",
    "    print(\"Restoring weights from %s\" %model_path)\n",
    "    saver.restore(sess,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "video_files = list(video_paths.keys())    \n",
    "    \n",
    "iter = 0\n",
    "btch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Processing batch %d\" %(int(btch/batch_size)+1))\n",
    "start = btch\n",
    "# end = min(len(video_files),btch+batch_size)\n",
    "end = btch+3\n",
    "dataset={}\n",
    "dataset[\"video\"] = np.asarray([np.load(video_paths[video_files[i]]) for i in range(start,end)])\n",
    "dataset[\"path\"] = [video_paths[video_files[i]] for i in range(start,end)]\n",
    "dataset[\"file\"] = [video_files[i] for i in range(start,end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "self = infer_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"gen_caption\"] = infer_util.generate_caption_batch(sess,dataset[\"video\"],max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset[\"gen_caption\"] = infer_util.generate_caption_batch_beam(sess,beam_size,dataset[\"video\"],max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset[\"gen_caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "video_batch = dataset[\"video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_batch = np.array([[self.word_to_idx[\"<bos>\"]]]*video_batch.shape[0])\n",
    "state = np.array(self.feed_video(sess,video_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "state = np.split(state,state.shape[2],axis=2)\n",
    "# state is list of size batch_size with each element of shape num_layer * 2 * 1 *hidden_size        \n",
    "batch_of_beams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(video_batch.shape[0]):\n",
    "    beam = [] # {st: , current_cap: , loss: , prev_word:}\n",
    "    #for j in range(beam_size):\n",
    "    beam.append({\"st\":state[i],\n",
    "                     \"current_cap\":\"\" ,\n",
    "                     \"loss\":0,\n",
    "                     \"prev_word\":self.word_to_idx[\"<bos>\"] })\n",
    "    batch_of_beams.append(beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "completed_captions = [[] for d in range(video_batch.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beam_squared_list = [[] for d in range(video_batch.shape[0])] \n",
    "for vv in range(len(batch_of_beams[0])):\n",
    "    input_batch = [[video[vv][\"prev_word\"]] for video in batch_of_beams]\n",
    "    state = [video[vv][\"st\"] for video in batch_of_beams] #batchsize * 2 * 2000\n",
    "    state = np.concatenate(state,axis=2)\n",
    "    pred,state = self.inference_step(sess,input_batch,state)\n",
    "    state = np.array(self.feed_video(sess,video_batch))\n",
    "    state = np.split(state,state.shape[2],axis=2)\n",
    "    pred = np.squeeze(pred,axis=1)\n",
    "    for j in range(len(beam_squared_list)):\n",
    "        #print(\"-------- Vid ------\",j, \" Current cap \", batch_of_beams[j][vv][\"current_cap\"] )\n",
    "        for pred_word in pred[j].argsort()[-beam_size:][::-1]:\n",
    "            #print(pred_word, \" Pred Word-- \", self.idx_to_word[pred_word])\n",
    "            new_loss = batch_of_beams[j][vv][\"loss\"] - np.log(pred[j][pred_word])\n",
    "            if (pred_word == self.word_to_idx[\"<eos>\"]) : \n",
    "                completed_captions[j].append(( new_loss / (i+1) , # did  +1 avoiding divide by 0\n",
    "                                              batch_of_beams[j][vv][\"current_cap\"]))\n",
    "            else:\n",
    "                beam_squared_list[j].append({\"st\":state[j] ,\n",
    "                                             \"current_cap\":batch_of_beams[j][vv][\"current_cap\"] + \n",
    "                                                \" \" + self.idx_to_word[pred_word],\n",
    "                                             \"loss\":new_loss,\n",
    "                                             \"prev_word\":pred_word })\n",
    "\n",
    "for j in range(len(beam_squared_list)):\n",
    "    beam_squared_list[j].sort(key = lambda x: x[\"loss\"])\n",
    "    batch_of_beams[j] = beam_squared_list[j][:beam_size].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
