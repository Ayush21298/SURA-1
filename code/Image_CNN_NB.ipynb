{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nets.inception_v4 import inception_v4_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inception_v4_mod(images,\n",
    "                 trainable=True,\n",
    "                 is_training=True,\n",
    "                 weight_decay=0.00004,\n",
    "                 stddev=0.1,\n",
    "                 dropout_keep_prob=0.8,\n",
    "                 use_batch_norm=True,\n",
    "                 batch_norm_params=None,\n",
    "                 add_summaries=True,\n",
    "                 scope=\"InceptionV4\"):\n",
    "  \"\"\"Builds an Inception V3 subgraph for image embeddings.\n",
    "\n",
    "  Args:\n",
    "    images: A float32 Tensor of shape [batch, height, width, channels].\n",
    "    trainable: Whether the inception submodel should be trainable or not.\n",
    "    is_training: Boolean indicating training mode or not.\n",
    "    weight_decay: Coefficient for weight regularization.\n",
    "    stddev: The standard deviation of the trunctated normal weight initializer.\n",
    "    dropout_keep_prob: Dropout keep probability.\n",
    "    use_batch_norm: Whether to use batch normalization.\n",
    "    batch_norm_params: Parameters for batch normalization. See\n",
    "      tf.contrib.layers.batch_norm for details.\n",
    "    add_summaries: Whether to add activation summaries.\n",
    "    scope: Optional Variable scope.\n",
    "\n",
    "  Returns:\n",
    "    end_points: A dictionary of activations from inception_v3 layers.\n",
    "  \"\"\"\n",
    "  # Only consider the inception model to be in training mode if it's trainable.\n",
    "  is_inception_model_training = trainable and is_training\n",
    "\n",
    "  if use_batch_norm:\n",
    "    # Default parameters for batch normalization.\n",
    "    if not batch_norm_params:\n",
    "      batch_norm_params = {\n",
    "          \"is_training\": is_inception_model_training,\n",
    "          \"trainable\": trainable,\n",
    "          # Decay for the moving averages.\n",
    "          \"decay\": 0.9997,\n",
    "          # Epsilon to prevent 0s in variance.\n",
    "          \"epsilon\": 0.001,\n",
    "          # Collection containing the moving mean and moving variance.\n",
    "          \"variables_collections\": {\n",
    "              \"beta\": None,\n",
    "              \"gamma\": None,\n",
    "              \"moving_mean\": [\"moving_vars\"],\n",
    "              \"moving_variance\": [\"moving_vars\"],\n",
    "          }\n",
    "      }\n",
    "  else:\n",
    "    batch_norm_params = None\n",
    "\n",
    "  if trainable:\n",
    "    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n",
    "  else:\n",
    "    weights_regularizer = None\n",
    "\n",
    "  with tf.variable_scope(scope, \"InceptionV4\", [images]) as scope:\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d, slim.fully_connected],\n",
    "        weights_regularizer=weights_regularizer,\n",
    "        trainable=trainable):\n",
    "      with slim.arg_scope(\n",
    "          [slim.conv2d],\n",
    "          weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "          activation_fn=tf.nn.relu,\n",
    "          normalizer_fn=slim.batch_norm,\n",
    "          normalizer_params=batch_norm_params):\n",
    "        net, end_points = inception_v4_base(images, scope=scope)\n",
    "        with tf.variable_scope(\"logits\"):\n",
    "          shape = net.get_shape()\n",
    "          net = slim.avg_pool2d(net, shape[1:3], padding=\"VALID\", scope=\"pool\")\n",
    "          net = slim.dropout(\n",
    "              net,\n",
    "              keep_prob=dropout_keep_prob,\n",
    "              is_training=is_inception_model_training,\n",
    "              scope=\"dropout\")\n",
    "          net = slim.flatten(net, scope=\"flatten\")\n",
    "\n",
    "  # Add summaries.\n",
    "  if add_summaries:\n",
    "    for v in end_points.values():\n",
    "      tf.contrib.layers.summaries.summarize_activation(v)\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inception_v3_mod(images,\n",
    "                 trainable=True,\n",
    "                 is_training=True,\n",
    "                 weight_decay=0.00004,\n",
    "                 stddev=0.1,\n",
    "                 dropout_keep_prob=0.8,\n",
    "                 use_batch_norm=True,\n",
    "                 batch_norm_params=None,\n",
    "                 add_summaries=True,\n",
    "                 scope=\"InceptionV3\"):\n",
    "  \"\"\"Builds an Inception V3 subgraph for image embeddings.\n",
    "\n",
    "  Args:\n",
    "    images: A float32 Tensor of shape [batch, height, width, channels].\n",
    "    trainable: Whether the inception submodel should be trainable or not.\n",
    "    is_training: Boolean indicating training mode or not.\n",
    "    weight_decay: Coefficient for weight regularization.\n",
    "    stddev: The standard deviation of the trunctated normal weight initializer.\n",
    "    dropout_keep_prob: Dropout keep probability.\n",
    "    use_batch_norm: Whether to use batch normalization.\n",
    "    batch_norm_params: Parameters for batch normalization. See\n",
    "      tf.contrib.layers.batch_norm for details.\n",
    "    add_summaries: Whether to add activation summaries.\n",
    "    scope: Optional Variable scope.\n",
    "\n",
    "  Returns:\n",
    "    end_points: A dictionary of activations from inception_v3 layers.\n",
    "  \"\"\"\n",
    "  # Only consider the inception model to be in training mode if it's trainable.\n",
    "  is_inception_model_training = trainable and is_training\n",
    "\n",
    "  if use_batch_norm:\n",
    "    # Default parameters for batch normalization.\n",
    "    if not batch_norm_params:\n",
    "      batch_norm_params = {\n",
    "          \"is_training\": is_inception_model_training,\n",
    "          \"trainable\": trainable,\n",
    "          # Decay for the moving averages.\n",
    "          \"decay\": 0.9997,\n",
    "          # Epsilon to prevent 0s in variance.\n",
    "          \"epsilon\": 0.001,\n",
    "          # Collection containing the moving mean and moving variance.\n",
    "          \"variables_collections\": {\n",
    "              \"beta\": None,\n",
    "              \"gamma\": None,\n",
    "              \"moving_mean\": [\"moving_vars\"],\n",
    "              \"moving_variance\": [\"moving_vars\"],\n",
    "          }\n",
    "      }\n",
    "  else:\n",
    "    batch_norm_params = None\n",
    "\n",
    "  if trainable:\n",
    "    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n",
    "  else:\n",
    "    weights_regularizer = None\n",
    "\n",
    "  with tf.variable_scope(scope, \"InceptionV3\", [images]) as scope:\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d, slim.fully_connected],\n",
    "        weights_regularizer=weights_regularizer,\n",
    "        trainable=trainable):\n",
    "      with slim.arg_scope(\n",
    "          [slim.conv2d],\n",
    "          weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n",
    "          activation_fn=tf.nn.relu,\n",
    "          normalizer_fn=slim.batch_norm,\n",
    "          normalizer_params=batch_norm_params):\n",
    "        net, end_points = inception_v3_base(images, scope=scope)\n",
    "        with tf.variable_scope(\"logits\"):\n",
    "          shape = net.get_shape()\n",
    "          net = slim.avg_pool2d(net, shape[1:3], padding=\"VALID\", scope=\"pool\")\n",
    "          net = slim.dropout(\n",
    "              net,\n",
    "              keep_prob=dropout_keep_prob,\n",
    "              is_training=is_inception_model_training,\n",
    "              scope=\"dropout\")\n",
    "          net = slim.flatten(net, scope=\"flatten\")\n",
    "\n",
    "  # Add summaries.\n",
    "  if add_summaries:\n",
    "    for v in end_points.values():\n",
    "      tf.contrib.layers.summaries.summarize_activation(v)\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model(images):\n",
    "    inception_output = inception_v4_mod(images,trainable=False)\n",
    "    return inception_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_embedding(base_output,output_units=1000):\n",
    "    with tf.variable_scope(\"image_embedding\") as scope:\n",
    "      image_embeddings = tf.contrib.layers.fully_connected(\n",
    "          inputs=base_output,\n",
    "          num_outputs=output_units,\n",
    "          activation_fn=None,\n",
    "          biases_initializer=None,\n",
    "          scope=scope)\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_feed = tf.placeholder(tf.float32,[None,299,299,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " inception_output = get_base_model(image_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fine_tune_input = tf.placeholder_with_default(inception_output,inception_output.get_shape(),name=\"fine_tune_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fine_tune_input:0' shape=(?, 1536) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    inception_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"InceptionV4\")\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver(var_list=inception_variables)\n",
    "    saver.restore(sess,\"./inception_v4.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
