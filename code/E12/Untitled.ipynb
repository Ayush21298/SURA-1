{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --batch_size: conflicting option string: --batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f40585e06903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m tf.flags.DEFINE_integer(\"batch_size\", 64,\n\u001b[0;32m---> 22\u001b[0;31m                        \"Batch size of train data input.\")\n\u001b[0m\u001b[1;32m     23\u001b[0m tf.flags.DEFINE_integer(\"beam_size\", 3,\n\u001b[1;32m     24\u001b[0m                        \"Beam size.\")\n",
      "\u001b[0;32m/home/ozym4nd145/.local/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m   \u001b[0m_define_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ozym4nd145/.local/lib/python3.5/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m_define_helper\u001b[0;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                               type=flagtype)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;31m# resolve any conflicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# add to actions list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             \u001b[0mconflict_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[0;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[1;32m   1513\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m                                      in conflicting_actions])\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --batch_size: conflicting option string: --batch_size"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "from model import Caption_Model\n",
    "from data_generator import Data_Generator\n",
    "from inference_util import Inference\n",
    "\n",
    "import inception_base\n",
    "import configuration\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                       \"Batch size of train data input.\")\n",
    "tf.flags.DEFINE_integer(\"beam_size\", 3,\n",
    "                       \"Beam size.\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_model\", None,\n",
    "                       \"Model Checkpoint to use.\")\n",
    "tf.flags.DEFINE_integer(\"max_captions\", None,\n",
    "                       \"Maximum number of captions to generate\")\n",
    "tf.flags.DEFINE_integer(\"max_len_captions\", None,\n",
    "                       \"Maximum length of captions to generate\")\n",
    "tf.flags.DEFINE_string(\"dataset\", \"test\",\n",
    "                       \"Dataset to use\")\n",
    "tf.flags.DEFINE_string(\"outfile_name\", \"generated_caption.json\",\n",
    "                       \"Name of the output result file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "beam_size = 3\n",
    "checkpoint_model = \"./models/train/model-21160\"\n",
    "max_captions = None\n",
    "max_len_captions = 30\n",
    "dataset = \"test\"\n",
    "outfile_name = \"generated_caption.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_config = configuration.DataConfig().config\n",
    "data_gen = Data_Generator(processed_video_dir = data_config[\"processed_video_dir\"],\n",
    "                        caption_file = data_config[\"caption_file\"],\n",
    "                        unique_freq_cutoff = data_config[\"unique_frequency_cutoff\"],\n",
    "                        max_caption_len = data_config[\"max_caption_length\"])\n",
    "\n",
    "data_gen.load_vocabulary(data_config[\"caption_data_dir\"])\n",
    "data_gen.load_dataset(data_config[\"caption_data_dir\"])\n",
    "\n",
    "assert dataset in [\"val\",\"test\",\"train\"]\n",
    "\n",
    "if max_len_captions:\n",
    "    max_len = max_len_captions\n",
    "else:\n",
    "    max_len = data_config['max_caption_length']\n",
    "\n",
    "model_config = configuration.ModelConfig(data_gen).config\n",
    "model = Caption_Model( num_frames = model_config[\"num_frames\"],\n",
    "                    image_width = model_config[\"image_width\"],\n",
    "                    image_height = model_config[\"image_height\"],\n",
    "                    image_channels = model_config[\"image_channels\"],\n",
    "                    num_caption_unroll = model_config[\"num_caption_unroll\"],\n",
    "                    num_last_layer_units = model_config[\"num_last_layer_units\"],\n",
    "                    image_embedding_size = model_config[\"image_embedding_size\"],\n",
    "                    word_embedding_size = model_config[\"word_embedding_size\"],\n",
    "                    hidden_size_lstm = model_config[\"hidden_size_lstm\"],\n",
    "                    num_lstm_layer = model_config[\"num_lstm_layer\"],\n",
    "                    vocab_size = model_config[\"vocab_size\"],\n",
    "                    initializer_scale = model_config[\"initializer_scale\"],\n",
    "                    learning_rate = model_config[\"learning_rate\"],\n",
    "                    mode=\"inference\",\n",
    "                    rnn1_input_keep_prob=model_config[\"rnn1_input_keep_prob\"],\n",
    "                    rnn1_output_keep_prob=model_config[\"rnn1_output_keep_prob\"],\n",
    "                    rnn2_input_keep_prob=model_config[\"rnn2_input_keep_prob\"],\n",
    "                    rnn2_output_keep_prob=model_config[\"rnn2_output_keep_prob\"]\n",
    "                    )\n",
    "model.build()\n",
    "\n",
    "infer_util = Inference(model,data_gen.word_to_idx,data_gen.idx_to_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if max_captions:\n",
    "    max_iter = max_captions\n",
    "else:\n",
    "    max_iter = len(data_gen.dataset[dataset])+10 #+10 is just to be safe ;)\n",
    "\n",
    "video_paths = {i[\"file_name\"]:i[\"path\"] for i in data_gen.dataset[dataset]}\n",
    "\n",
    "\n",
    "gen_captions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./models/train/model-21160\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gen_caption = []    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if model_path != None:\n",
    "    print(\"Restoring weights from %s\" %model_path)\n",
    "    saver.restore(sess,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "video_files = list(video_paths.keys())    \n",
    "    \n",
    "iter = 0\n",
    "btch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Processing batch %d\" %(int(btch/batch_size)+1))\n",
    "start = btch\n",
    "# end = min(len(video_files),btch+batch_size)\n",
    "end = btch+3\n",
    "dataset={}\n",
    "dataset[\"video\"] = np.asarray([np.load(video_paths[video_files[i]]) for i in range(start,end)])\n",
    "dataset[\"path\"] = [video_paths[video_files[i]] for i in range(start,end)]\n",
    "dataset[\"file\"] = [video_files[i] for i in range(start,end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "self = infer_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset[\"gen_caption\"] = infer_util.generate_caption_batch(sess,dataset[\"video\"],max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset[\"gen_caption\"] = infer_util.generate_caption_batch_beam(sess,beam_size,dataset[\"video\"],max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset[\"gen_caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "video_batch = dataset[\"video\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_batch = np.array([[self.word_to_idx[\"<bos>\"]]]*video_batch.shape[0])\n",
    "state = np.array(self.feed_video(sess,video_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "state = np.split(state,state.shape[2],axis=2)\n",
    "# state is list of size batch_size with each element of shape num_layer * 2 * 1 *hidden_size        \n",
    "batch_of_beams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(video_batch.shape[0]):\n",
    "    beam = [] # {st: , current_cap: , loss: , prev_word:}\n",
    "    #for j in range(beam_size):\n",
    "    beam.append({\"st\":state[i],\n",
    "                     \"current_cap\":\"\" ,\n",
    "                     \"loss\":0,\n",
    "                     \"prev_word\":self.word_to_idx[\"<bos>\"] })\n",
    "    batch_of_beams.append(beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "completed_captions = [[] for d in range(video_batch.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "beam_squared_list = [[] for d in range(video_batch.shape[0])] \n",
    "for vv in range(len(batch_of_beams[0])):\n",
    "    input_batch = [[video[vv][\"prev_word\"]] for video in batch_of_beams]\n",
    "    state = [video[vv][\"st\"] for video in batch_of_beams] #batchsize * 2 * 2000\n",
    "    state = np.concatenate(state,axis=2)\n",
    "    pred,state = self.inference_step(sess,input_batch,state)\n",
    "    state = np.array(self.feed_video(sess,video_batch))\n",
    "    state = np.split(state,state.shape[2],axis=2)\n",
    "    pred = np.squeeze(pred,axis=1)\n",
    "    for j in range(len(beam_squared_list)):\n",
    "        #print(\"-------- Vid ------\",j, \" Current cap \", batch_of_beams[j][vv][\"current_cap\"] )\n",
    "        for pred_word in pred[j].argsort()[-beam_size:][::-1]:\n",
    "            #print(pred_word, \" Pred Word-- \", self.idx_to_word[pred_word])\n",
    "            new_loss = batch_of_beams[j][vv][\"loss\"] - np.log(pred[j][pred_word])\n",
    "            if (pred_word == self.word_to_idx[\"<eos>\"]) : \n",
    "                completed_captions[j].append(( new_loss / (i+1) , # did  +1 avoiding divide by 0\n",
    "                                              batch_of_beams[j][vv][\"current_cap\"]))\n",
    "            else:\n",
    "                beam_squared_list[j].append({\"st\":state[j] ,\n",
    "                                             \"current_cap\":batch_of_beams[j][vv][\"current_cap\"] + \n",
    "                                                \" \" + self.idx_to_word[pred_word],\n",
    "                                             \"loss\":new_loss,\n",
    "                                             \"prev_word\":pred_word })\n",
    "\n",
    "for j in range(len(beam_squared_list)):\n",
    "    beam_squared_list[j].sort(key = lambda x: x[\"loss\"])\n",
    "    batch_of_beams[j] = beam_squared_list[j][:beam_size].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
