## SURA Ideas
1. Use dropout wrapper in training
2. Use multilayered rnn cell
3. try single layered approach of sequence to sequence
4. Adjust hidden units of lstm (maybe try 512 or 256)
5. Try pre configured word embedding (like gloVe)
6. Batch normalization
7. Regularization in weights of Dense layer
8. Glorot Initialization?
9. Gradient Clipping?
10. 